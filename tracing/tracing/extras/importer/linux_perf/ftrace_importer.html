<!DOCTYPE html>
<!--
Copyright (c) 2012 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->

<link rel="import" href="/tracing/base/color_scheme.html">
<link rel="import" href="/tracing/base/trace_stream.html">
<link rel="import" href="/tracing/base/utils.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/android_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/binder_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/bus_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/clock_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/cpufreq_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/disk_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/dma_fence_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/drm_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/exynos_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/gesture_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/i2c_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/spi_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/i915_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/ion_heap_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/irq_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/cobalt_irq_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/kfunc_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/mali_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/memreclaim_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/msm_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/power_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/regulator_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/pm_qos_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/runtime_pm_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/rss_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/sched_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/cobalt_sched_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/sync_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/thermal_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/wakeup_source_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/workqueue_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/net_parser.html">
<link rel="import" href="/tracing/extras/importer/linux_perf/qdisc_parser.html">
<link rel="import" href="/tracing/importer/importer.html">
<link rel="import" href="/tracing/importer/simple_line_reader.html">
<link rel="import" href="/tracing/model/clock_sync_manager.html">
<link rel="import" href="/tracing/model/model.html">

<script>
/**
 * @fileoverview Imports text files in the Linux event trace format into the
 * Model. This format is output both by sched_trace and by Linux's perf tool.
 *
 * This importer assumes the events arrive as a string. The unit tests provide
 * examples of the trace format.
 *
 * Linux scheduler traces use a definition for 'pid' that is different than
 * tracing uses. Whereas tracing uses pid to identify a specific process, a pid
 * in a linux trace refers to a specific thread within a process. Within this
 * file, we the definition used in Linux traces, as it improves the importing
 * code's readability.
 */
'use strict';

tr.exportTo('tr.e.importer.linux_perf', function() {
  const MONOTONIC_TO_FTRACE_GLOBAL_SYNC_ID =
      'linux_clock_monotonic_to_ftrace_global';
  const PTP_TO_FTRACE_GLOBAL_SYNC_ID =
      'linux_clock_ptp_to_ftrace_global';
  const ColorScheme = tr.b.ColorScheme;      

  const IMPORT_PRIORITY = 2;

  /**
   * Imports linux perf events into a specified model.
   * @constructor
   */
  function FTraceImporter(model, events) {
    this.importPriority = IMPORT_PRIORITY;
    this.model_ = model;
    this.events_ = events;
    this.xnSync_ = [];
    this.lostage_ = [];
    this.sleepons_ = [];
    this.gohard_ = [];
    this.gorelax_ = [];
    this.hardened_ = [];
    this.relaxed_ = [];
    this.wakeups_ = [];
    this.netqueues_ = [];
    this.netxmits_ = [];
    this.nettimeouts_ = [];
    this.netrecvs_ = [];
    this.netirqs_ = [];
    this.netrxirqs_ = {};
    this.nettxirqs_ = [];
    this.blockedReasons_ = [];
    this.kernelThreadStates_ = {};
    this.xnThreadStates_ = {};
    this.userThreadStates_ = {};
    this.buildMapFromLinuxPidsToThreads_();
    this.threadsByIrqs = {};
    this.lines_ = [];
    this.pseudoThreadCounter = -1;
    this.parsers_ = [];
    this.eventHandlers_ = {};
    this.haveClockSyncedMonotonicToGlobal_ = false;
    this.haveClockSyncedPtpToGlobal_ = false;
    this.clockDomainId_ = tr.model.ClockDomainId.LINUX_FTRACE_GLOBAL;
    const KernelModelHelper = tr.model.helpers.KernelModelHelper;

    this.prevHeadSchedState = "-";
    this.prevRootSchedState = "-";
  }

  const TestExports = {};

  // Matches the trace record in 3.2 and later with the print-tgid option:
  // withTGID
  //          <idle>-0     [000] (-----) dN..  2484.980174: sched_wakeup: comm=ktimersoftd/0 pid=8 prio=98 target_cpu=000
  // withTGIDpatch
  //          <idle>-0     (-----) [001] dN..   905.243560: sched_wakeup: comm=ksoftirqd/1 pid=20 prio=120 target_cpu=001
  // A TGID (Thread Group ID) is basically what the Linux kernel calls what
  // userland refers to as a process ID (as opposed to a Linux pid, which is
  // what userland calls a thread ID).
  const lineREWithTGID = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\[(\\d+)\\]\\s\\(\\s*(\\d+|-+)\\)' +
      '\\s+[dX.][Nnp.][Hhs.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const lineParserWithTGID = function(line) {
    const groups = lineREWithTGID.exec(line);
    if (!groups) return groups;

    let tgid = groups[4];
    if (tgid[0] === '-') tgid = undefined;

    return {
      threadName: groups[1],
      pid: groups[2],
      tgid,
      cpuNumber: groups[3],
      timestamp: groups[5],
      eventName: groups[6],
      details: groups[7]
    };
  };
  TestExports.lineParserWithTGID = lineParserWithTGID;

  const lineREWithTGIDpatch = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\(\\s*(\\d+|-+)\\)\\s\\[(\\d+)\\]' +
      '\\s+[dX.][Nnp.][Hhs.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const lineParserWithTGIDpatch = function(line) {
    const groups = lineREWithTGIDpatch.exec(line);
    if (!groups) return groups;

    let tgid = groups[3];
    if (tgid[0] === '-') tgid = undefined;

    return {
      threadName: groups[1],
      pid: groups[2],
      tgid,
      cpuNumber: groups[4],
      timestamp: groups[5],
      eventName: groups[6],
      details: groups[7]
    };
  };
  TestExports.lineParserWithTGIDpatch = lineParserWithTGIDpatch;

  // Matches the default trace record in 3.2 and later (includes irq-info):
  //          <idle>-0     [001] d...  1.23: sched_switch
  const lineREWithIRQInfo = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\[(\\d+)\\]' +
      '\\s+[dX.][Nnp.][Hhs.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const lineParserWithIRQInfo = function(line) {
    const groups = lineREWithIRQInfo.exec(line);
    if (!groups) return groups;
    return {
      threadName: groups[1],
      pid: groups[2],
      cpuNumber: groups[3],
      timestamp: groups[4],
      eventName: groups[5],
      details: groups[6]
    };
  };
  TestExports.lineParserWithIRQInfo = lineParserWithIRQInfo;

  // PREMPT_RT : Matches the trace record in 3.2 and later with the print-tgid option:
  // withTGID
  //          <idle>-0     [000] (-----) dN..3..  2484.980174: sched_wakeup: comm=ktimersoftd/0 pid=8 prio=98 target_cpu=000
  // withTGIDpatch
  //          <idle>-0     (-----) [001] dN..3..   905.243560: sched_wakeup: comm=ksoftirqd/1 pid=20 prio=120 target_cpu=001
  // A TGID (Thread Group ID) is basically what the Linux kernel calls what
  // userland refers to as a process ID (as opposed to a Linux pid, which is
  // what userland calls a thread ID).
  const PREEMPT_RTlineREWithTGID = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\[(\\d+)\\]\\s\\(\\s*(\\d+|-+)\\)' +
      '\\s+[dX.][Nnp.][NnLp.][Hhs.][0-9a-f.][0-9a-f.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const PREEMPT_RTlineParserWithTGID = function(line) {
    const groups = PREEMPT_RTlineREWithTGID.exec(line);
    if (!groups) return groups;

    let tgid = groups[4];
    if (tgid[0] === '-') tgid = undefined;

    return {
      threadName: groups[1],
      pid: groups[2],
      tgid,
      cpuNumber: groups[3],
      timestamp: groups[5],
      eventName: groups[6],
      details: groups[7]
    };
  };
  TestExports.PREEMPT_RTlineParserWithTGID = PREEMPT_RTlineParserWithTGID;

  const PREEMPT_RTlineREWithTGIDpatch = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\(\\s*(\\d+|-+)\\)\\s\\[(\\d+)\\]' +
      '\\s+[dX.][Nnp.][NnLp.][Hhs.][0-9a-f.][0-9a-f.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const PREEMPT_RTlineParserWithTGIDpatch = function(line) {
    const groups = PREEMPT_RTlineREWithTGIDpatch.exec(line);
    if (!groups) return groups;

    let tgid = groups[3];
    if (tgid[0] === '-') tgid = undefined;

    return {
      threadName: groups[1],
      pid: groups[2],
      tgid,
      cpuNumber: groups[4],
      timestamp: groups[5],
      eventName: groups[6],
      details: groups[7]
    };
  };
  TestExports.PREEMPT_RTlineParserWithTGIDpatch = PREEMPT_RTlineParserWithTGIDpatch;

  // Matches the default trace record in 3.2 and later (includes irq-info):
  //          <idle>-0     [001] d...  1.23: sched_switch
  const PREEMPT_RTlineREWithIRQInfo = new RegExp(
      '^\\s*(.+)-(\\d+)\\s+\\[(\\d+)\\]' +
      '\\s+[dX.][Nnp.][NnLp.][Hhs.][0-9a-f.][0-9a-f.][0-9a-f.]' +
      '\\s+(\\d+\\.\\d+):\\s+(\\S+):\\s(.*)$');
  const PREEMPT_RTlineParserWithIRQInfo = function(line) {
    const groups = PREEMPT_RTlineREWithIRQInfo.exec(line);
    if (!groups) return groups;
    return {
      threadName: groups[1],
      pid: groups[2],
      cpuNumber: groups[3],
      timestamp: groups[4],
      eventName: groups[5],
      details: groups[6]
    };
  };
  TestExports.PREEMPT_RTlineParserWithIRQInfo = PREEMPT_RTlineParserWithIRQInfo;

  // Matches the default trace record pre-3.2:
  //          <idle>-0     [001]  1.23: sched_switch
  const lineREWithLegacyFmt =
      /^\s*(.+)-(\d+)\s+\[(\d+)\]\s*(\d+\.\d+):\s+(\S+):\s(.*)$/;
  const lineParserWithLegacyFmt = function(line) {
    const groups = lineREWithLegacyFmt.exec(line);
    if (!groups) {
      return groups;
    }
    return {
      threadName: groups[1],
      pid: groups[2],
      cpuNumber: groups[3],
      timestamp: groups[4],
      eventName: groups[5],
      details: groups[6]
    };
  };
  TestExports.lineParserWithLegacyFmt = lineParserWithLegacyFmt;

  // Matches the trace_event_clock_sync marker:
  //  0: trace_event_clock_sync: parent_ts=19581477508
  const traceEventClockSyncRE = /trace_event_clock_sync: parent_ts=(\d+\.?\d*)/;
  TestExports.traceEventClockSyncRE = traceEventClockSyncRE;

  // Matches the trace_event_clock_sync marker:
  // 0: trace_event_clock_sync: parent_ts=1525853974.18952 seq=64884 tai_lat=15850 ep=enp1s0
  const traceEventPtpClockSyncRE = /trace_event_clock_sync: parent_ts=(\d+)\.(\d+) seq=(\d+) tai_lat=(\d+) ep=(\S+)/;
  TestExports.traceEventPtpClockSyncRE = traceEventPtpClockSyncRE;

  const realTimeClockSyncRE = /trace_event_clock_sync: realtime_ts=(\d+)/;
  const genericClockSyncRE = /trace_event_clock_sync: name=([\w\-]+)/;

  // Some kernel trace events are manually classified in slices and
  // hand-assigned a pseudo PID.
  const pseudoKernelPID = 'Others';

  /**
   * Deduce the format of trace data. Linux kernels prior to 3.3 used one
   * format (by default); 3.4 and later used another.  Additionally, newer
   * kernels can optionally trace the TGID.
   *
   * @return {function} the function for parsing data when the format is
   * recognized; otherwise undefined.
   */
  function autoDetectLineParser(line) {
    if (line[0] === '{') return false;
    if (lineREWithTGID.test(line)) return lineParserWithTGID;
    if (lineREWithTGIDpatch.test(line)) return lineParserWithTGIDpatch;
    if (lineREWithIRQInfo.test(line)) return lineParserWithIRQInfo;
    if (PREEMPT_RTlineREWithTGID.test(line)) return PREEMPT_RTlineParserWithTGID;
    if (PREEMPT_RTlineREWithTGIDpatch.test(line)) return PREEMPT_RTlineParserWithTGIDpatch;
    if (PREEMPT_RTlineREWithIRQInfo.test(line)) return PREEMPT_RTlineParserWithIRQInfo;
    if (lineREWithLegacyFmt.test(line)) return lineParserWithLegacyFmt;
    return undefined;
  }
  TestExports.autoDetectLineParser = autoDetectLineParser;

  /**
   * Guesses whether the provided events is a Linux perf string.
   * Looks for the magic string "# tracer" at the start of the file,
   * or the typical task-pid-cpu-timestamp-function sequence of a typical
   * trace's body.
   *
   * @return {boolean} True when events is a linux perf array.
   */
  FTraceImporter.canImport = function(events) {
    if (events instanceof tr.b.TraceStream) events = events.header;

    if (!(typeof(events) === 'string' || events instanceof String)) {
      return false;
    }

    if (FTraceImporter._extractEventsFromSystraceHTML(events, false).ok) {
      return true;
    }

    if (FTraceImporter._extractEventsFromSystraceMultiHTML(events, false).ok) {
      return true;
    }

    if (/^# tracer:/.test(events)) return true;

    const lineBreakIndex = events.indexOf('\n');
    if (lineBreakIndex > -1) events = events.substring(0, lineBreakIndex);

    if (autoDetectLineParser(events)) return true;

    return false;
  };

  FTraceImporter._extractEventsFromSystraceHTML = function(
      incomingEvents, produceResult) {
    const failure = {ok: false};
    if (produceResult === undefined) produceResult = true;

    const header = incomingEvents instanceof tr.b.TraceStream ?
      incomingEvents.header : incomingEvents;
    if (!/^<!DOCTYPE html>/.test(header)) return failure;
    const r = new tr.importer.SimpleLineReader(incomingEvents);

    // Try to find the data...
    if (!r.advanceToLineMatching(/^  <script>$/)) return failure;
    if (!r.advanceToLineMatching(/^  var linuxPerfData = "\\$/)) return failure;

    const eventsBeginAtLine = r.curLineNumber + 1;
    r.beginSavingLines();
    if (!r.advanceToLineMatching(/^  <\/script>$/)) return failure;

    let rawEvents = r.endSavingLinesAndGetResult();

    // Drop off first and last event as it contains the tag.
    rawEvents = rawEvents.slice(1, rawEvents.length - 1);

    if (!r.advanceToLineMatching(/^<\/body>$/)) return failure;
    if (!r.advanceToLineMatching(/^<\/html>$/)) return failure;

    function endsWith(str, suffix) {
      return str.indexOf(suffix, str.length - suffix.length) !== -1;
    }
    function stripSuffix(str, suffix) {
      if (!endsWith(str, suffix)) return str;
      return str.substring(str, str.length - suffix.length);
    }

    // Strip off escaping in the file needed to preserve linebreaks.
    let events = [];
    if (produceResult) {
      for (let i = 0; i < rawEvents.length; i++) {
        let event = rawEvents[i];
        event = stripSuffix(event, '\\n\\');
        events.push(event);
      }
    } else {
      events = [rawEvents[rawEvents.length - 1]];
    }

    // Last event ends differently. Strip that off too,
    // treating absence of that trailing string as a failure.
    const oldLastEvent = events[events.length - 1];
    const newLastEvent = stripSuffix(oldLastEvent, '\\n";');
    if (newLastEvent === oldLastEvent) return failure;
    events[events.length - 1] = newLastEvent;

    return {ok: true,
      lines: produceResult ? events : undefined,
      eventsBeginAtLine};
  };

  FTraceImporter._extractEventsFromSystraceMultiHTML = function(
      incomingEvents, produceResult) {
    const failure = {ok: false};
    if (produceResult === undefined) produceResult = true;

    const header = incomingEvents instanceof tr.b.TraceStream ?
      incomingEvents.header : incomingEvents;
    if (!(new RegExp('^<!DOCTYPE HTML>', 'i').test(header))) return failure;

    const r = new tr.importer.SimpleLineReader(incomingEvents);

    // Try to find the Linux perf trace in any of the trace-data tags
    let events = [];
    let eventsBeginAtLine;
    while (!/^# tracer:/.test(events)) {
      if (!r.advanceToLineMatching(
          /^  <script class="trace-data" type="application\/text">$/)) {
        return failure;
      }

      eventsBeginAtLine = r.curLineNumber + 1;

      r.beginSavingLines();
      if (!r.advanceToLineMatching(/^  <\/script>$/)) return failure;

      events = r.endSavingLinesAndGetResult();

      // Drop off first and last event as it contains the tag.
      events = events.slice(1, events.length - 1);
    }

    if (!r.advanceToLineMatching(/^<\/body>$/)) return failure;
    if (!r.advanceToLineMatching(/^<\/html>$/)) return failure;

    return {
      ok: true,
      lines: produceResult ? events : undefined,
      eventsBeginAtLine,
    };
  };

  FTraceImporter.prototype = {
    __proto__: tr.importer.Importer.prototype,

    get importerName() {
      return 'FTraceImporter';
    },

    get model() {
      return this.model_;
    },

    /**
     * Imports clock sync markers into model_.
     */
    importClockSyncMarkers() {
      this.lazyInit_();
      this.forEachLine_(function(text, eventBase, cpuNumber, pid, ts) {
        const eventName = eventBase.eventName;
        if (eventName !== 'tracing_mark_write' && eventName !== '0') return;

        if (traceEventPtpClockSyncRE.exec(eventBase.details)) {
          this.traceClockPtpSyncEvent_(eventName, cpuNumber, pid, ts, eventBase);
        } else if (traceEventClockSyncRE.exec(eventBase.details) ||
            genericClockSyncRE.exec(eventBase.details)) {
          this.traceClockSyncEvent_(eventName, cpuNumber, pid, ts, eventBase);
        } else if (realTimeClockSyncRE.exec(eventBase.details)) {
          // TODO(charliea): Migrate this sync to ClockSyncManager.
          // This entry syncs CLOCK_REALTIME with CLOCK_MONOTONIC. Store the
          // offset between the two in the model so that importers parsing files
          // with CLOCK_REALTIME timestamps can map back to CLOCK_MONOTONIC.
          const match = realTimeClockSyncRE.exec(eventBase.details);
          this.model_.realtime_to_monotonic_offset_ms = ts - match[1];
        }
      }.bind(this));
    },

    /**
     * Imports the data in this.events_ into model_.
     */
    importEvents() {
      // On ChromeOS, the trace can sometimes be completely empty, lacking
      // even a clock sync marker. Exit early to avoid clock sync problems.
      if (this.lines_.length === 0) return;

      const modelTimeTransformer =
          this.model_.clockSyncManager.getModelTimeTransformer(
              this.clockDomainId_);

      this.importCpuData_(modelTimeTransformer);
      this.buildMapFromLinuxPidsToThreads_();
      this.buildPerThreadCpuSlicesFromCpuState_();
      this.buildPerThreadNetSlicesFromEthScope_();
    },

    /**
     * Registers a linux perf event parser used by importCpuData_.
     */
    registerEventHandler(eventName, handler) {
      // TODO(sleffler) how to handle conflicts?
      this.eventHandlers_[eventName] = handler;
    },

    /**
     * @return {Cpu} A Cpu corresponding to the given cpuNumber.
     */
    getOrCreateCpu(cpuNumber) {
      return this.model_.kernel.getOrCreateCpu(cpuNumber);
    },

    /**
     * @return {TimelineThread} A thread corresponding to the kernelThreadName.
     */
    getOrCreateKernelThread(kernelThreadName, pid, tid) {
      const key = parseInt(tid);
      if (!this.kernelThreadStates_[key]) {
        const thread = this.model_.getOrCreateProcess(pid).getOrCreateThread(
            tid);
        thread.name = kernelThreadName;

        if ( !thread.parent.name ) {
          const process = thread.parent;
          process.pid = 0;
          process.name = 'Kernel ['+ pid +']';
          process.userFriendlyName = 'Kernel ['+ pid +']';
          process.userFriendlyDetails = 'Kernel ['+ pid +']';
          process.sortIndex = Number.NEGATIVE_INFINITY;
          process.important = true; // auto expend
        }

        this.kernelThreadStates_[key] = {
          pid,
          thread,
          openSlice: undefined,
          openSliceTS: undefined,
          irqName: undefined,
          irqNum: undefined
        };
        this.threadsByLinuxPid[tid] = thread;
      }
      return this.kernelThreadStates_[key];
    },

    /**
     * Processes can have multiple threads.
     * thread names are not unique across processes we therefore need to
     * keep more information in order to return the correct threads.
     */
    getOrCreateUserThread(userThreadName, pid, tid) {
      const key = parseInt(tid);
      if (!this.userThreadStates_[key]) {
        const thread = this.model_.getOrCreateProcess(parseInt(pid)).getOrCreateThread(
            parseInt(tid));
        thread.name = userThreadName;

        if ( !thread.parent.name) {
          const process = thread.parent;
          process.name = userThreadName ;
          process.userFriendlyName = userThreadName ;
          process.sortIndex = parseInt(pid);
          process.important = false; // auto collapse
        }
        this.userThreadStates_[key] = {
          key,
          thread,
          openSlice: undefined,
          openSliceTS: undefined
        };
        this.threadsByLinuxPid[parseInt(tid)] = thread;
      }
      return this.userThreadStates_[key];
    },

    /**
     * Processes can have multiple Xenomai Cobalt xnthreads.
     * xnthreads thread names are not unique across processes we therefore need to
     * keep more information in order to return the correct threads.
     */
    getOrCreateXnThread(xnThreadName, pid, tid, xnid) {
      const key = xnid;
      if (!this.xnThreadStates_[key]) {
        const thread = this.model_.getOrCreatePseudoProcess(pid).getOrCreateThread(
            tid);

        thread.name = xnThreadName;
        thread.xnid = xnid;
        this.xnThreadStates_[key] = {
          pid,
          thread,
          lastarg: {stateWhenDescheduled: "-", xnStateWhenDescheduled: "-" },
          openSlice: undefined,
          openSliceTS: undefined
        };
        this.threadsByXnid[key] = thread;
        this.threadsByLinuxPid[pid] = thread;
      }
      return this.xnThreadStates_[key];
    },


    /**
     * Processes can have multiple binder threads.
     * Binder thread names are not unique across processes we therefore need to
     * keep more information in order to return the correct threads.
     */
    getOrCreateBinderKernelThread(kernelThreadName, pid, tid) {
      const key = kernelThreadName + pid + tid;
      if (!this.kernelThreadStates_[key]) {
        const thread = this.model_.getOrCreateProcess(pid).getOrCreateThread(
            tid);
        thread.name = kernelThreadName;
        this.kernelThreadStates_[key] = {
          pid,
          thread,
          openSlice: undefined,
          openSliceTS: undefined
        };
        this.threadsByLinuxPid[tid] = thread;
      }
      return this.kernelThreadStates_[key];
    },

    /**
     * @return {TimelineThread} A pseudo thread corresponding to the
     * kernelThreadName.  Pseudo threads are for events that we want to break
     * out to a separate timeline but would not otherwise happen.
     * These threads are assigned to user input TGID and given a
     * unique (incrementing) TID.
     */
    getOrCreatePseudoDeviceThread(kernelThreadName, pid) {
      const key = kernelThreadName+pid;
      if (!this.kernelThreadStates_[key]) {
        const tid = this.pseudoThreadCounter;
        const thread = this.model_.getOrCreatePseudoProcess(pid).getOrCreateThread(
            tid);
        thread.name = kernelThreadName;
        this.kernelThreadStates_[key] = {
          pid,
          thread,
          openSlice: undefined,
          openSliceTS: undefined,
          irqName: undefined,
          irqNum: undefined
        };

        if ( !thread.parent.name ) {
          const process = thread.parent;
          process.pid = 0;
          process.name = 'Kernel ['+ pid +']';
          process.userFriendlyName = 'Kernel ['+ pid +']';
          process.userFriendlyDetails = 'Kernel ['+ pid +']';
          process.sortIndex = Number.NEGATIVE_INFINITY;
          process.important = true; // auto expend
        }

        this.threadsByLinuxPid[tid] = thread;
        this.pseudoThreadCounter--;
      }
      return this.kernelThreadStates_[key];
    },

    /**
     * @return {TimelineThread} A pseudo thread corresponding to the
     * threadName.  Pseudo threads are for events that we want to break
     * out to a separate timeline but would not otherwise happen.
     * These threads are assigned to pseudoKernelPID and given a
     * unique (incrementing) TID.
     */
    getOrCreatePseudoThread(threadName) {
      let thread = this.kernelThreadStates_[threadName];
      if (!thread) {
        thread = this.getOrCreateKernelThread(threadName, pseudoKernelPID,
            this.pseudoThreadCounter);
        this.pseudoThreadCounter--;
      }
      return thread;
    },

    /**
     * @return {TimelineThread} A pseudo thread corresponding to the
     * threadName.  Pseudo Xenomai threads are for events that we want to break
     * out to a separate timeline but would not otherwise happen.
     * These threads are assigned to pseudoKernelPID and given a
     * unique (incrementing) TID.
     */
    getOrCreatePseudoXnThread(threadName,pid, xnid) {
      let thread = this.xnThreadStates_[xnid];
      if (!thread) {
        thread = this.getOrCreateXnThread(threadName, pid,
            -100+this.pseudoThreadCounter,xnid);
        this.pseudoThreadCounter--;
      }
      return thread;
    },

    /**
     * Records the fact that a pid has become runnable. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markPidRunnable(ts, pid, comm, prio, fromPid) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.wakeups_.push({ts, tid: pid, fromTid: fromPid, 'prio': prio});
    },

    /**
     * Records the fact that a pid has become runnable. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markPidLostage(ts, pid, comm, fromPid) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.lostage_.push({ts, tid: pid, fromTid: fromPid});
    },

  /*
   * Xenomai/Cobalt follows a strict declaration order of the state flags
   * defined above. Status symbols are defined as follows:
   *
   * 'S' -> Forcibly suspended.
   * 'w'/'W' -> Waiting for a resource, with or without timeout.
   * 'D' -> Delayed (without any other wait condition).
   * 'R' -> Runnable.
   * 'U' -> Unstarted or dormant.
   * 'X' -> Relaxed shadow.
   * 'H' -> Held in emergency.
   * 'b' -> Priority boost undergoing.
   * 'T' -> Ptraced and stopped.
   * 'l' -> Locks scheduler.
   * 'r' -> Undergoes round-robin.
   * 't' -> Runtime mode errors notified.
   * 'L' -> Lock breaks trapped.
   */
    formatXnThreadState(status) {
      const COBALT_CORE_STATE_MASK = tr.model.COBALT_CORE_STATE_MASK;
      const XNTHREAD_STATE_LABELS = "SWDRU..X.HbTlrt.....L.";
      const labels = XNTHREAD_STATE_LABELS;

      let c;
      let buf = '';

      for (var mask = status, pos = 0;
       mask != 0 ; 
       mask >>= 1, pos++) {

          if ((mask & 1) == 0)
              continue;

/* */
          c = labels.charAt(pos);

          switch (1 << pos) {
             case COBALT_CORE_STATE_MASK.XNROOT:
               c = 'R'; // Always mark root as runnable.
               break;

             case COBALT_CORE_STATE_MASK.XNREADY:
               if (status & COBALT_CORE_STATE_MASK.XNROOT)
                 continue; // Already reported on XNROOT.
               break;

             case COBALT_CORE_STATE_MASK.XNDELAY:
              // Only report genuine delays here, not timed waits for resources.
               if (status & COBALT_CORE_STATE_MASK.XNPEND)
                 continue;
               break;

             case COBALT_CORE_STATE_MASK.XNPEND:
               // Report timed waits with lowercase symbol.
               if (status & COBALT_CORE_STATE_MASK.XNDELAY)
                 c |= 0x20;
               break;

             default:
               if (c == '.')
                 continue;
          }
          buf += c;
      }
      return (buf);
    },


    /** Xenomai >= 3.0.9
     * Records the fact that a xnid has become Hardened. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidGohard(ts, pid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.gohard_.push({ts, tid: pid, 'xnstate': state, 'xninfo': info});
    },

    /** Xenomai < 3.0.9
     * Records the fact that a xnid has become Hardened. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidGohardLegacy(ts, pid, comm, xnid, fromPid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.gohard_.push({ts, tid: pid, fromTid: fromPid, xnid: xnid, 'xnstate': state, 'xninfo': info});
    },

    /**
     * Records the fact that a sync event as waken the thread. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnSyncWake(ts, xnsyncid, cpuid, fromPid) {
      // construction of the xenomai sync wakeup object.
      this.xnSync_.push({ts, xnsyncid: xnsyncid, cpuid: cpuid, fromTid: fromPid});
    },

    /**
     * Records the fact that a xnid goes into sleep on signal wake. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnSyncSleep(ts, xnid, xnsyncid, cpuid, fromPid) {
      // construction of the xenomai sync wakeup object.
      this.sleepons_.push({ts, xnid: xnid, xnsyncid: xnsyncid, cpuid: cpuid, fromTid: fromPid});
    },

    /**
     * Records the reason why a xnid has gone into uninterruptible sleep.
     */
    markXnidGorelaxReason(ts, xnid, fromPid, xnreason) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.gorelax_.push({ts, xnid: xnid, xnreason: xnreason});
    },
    /**
     * Records the reason why a xnid has gone into uninterruptible sleep.
     */
    markXnidGorelaxReasonLegacy(ts, xnid, xnreason) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.gorelax_.push({ts, xnid: xnid, xnreason});
    },    /**
     * Records the fact that a xnid has become Hardened. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidHardened(ts, xnid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.hardened_.push({ts, xnid: xnid, 'xnstate': state, 'xninfo': info});
    },
    /**
     * Records the fact that a xnid has become Hardened. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidHardenedLegacy(ts, pid, comm, xnid, fromPid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.hardened_.push({ts, tid: pid, fromTid: fromPid, xnid: xnid, 'xnstate': state, 'xninfo': info});
    },

    /**
     * Records the fact that a xnid has become relaxed. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidRelaxed(ts, xnid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.relaxed_.push({ts, xnid: xnid, 'xnstate': state, 'xninfo': info});
    },
    /**
     * Records the fact that a xnid has become relaxed. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markXnidRelaxedLegacy(ts, pid, comm, xnid, fromPid, state, info) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.relaxed_.push({ts, tid: pid, fromTid: fromPid, xnid: xnid, 'xnstate': state, 'xninfo': info});
    },
    /**
     * Records the reason why a pid has gone into uninterruptible sleep.
     */
    addPidBlockedReason(ts, pid, iowait, caller) {
      // The the pids that get passed in to this function are Linux kernel
      // pids, which identify threads.  The rest of trace-viewer refers to
      // these as tids, so the change of nomenclature happens in the following
      // construction of the wakeup object.
      this.blockedReasons_.push({ts, tid: pid, iowait,
        caller});
    },

    /**
     * Precomputes a lookup table from linux pids back to existing
     * Threads. This is used during importing to add information to each
     * thread about whether it was running, descheduled, sleeping, et
     * cetera.
     */
    buildMapFromLinuxPidsToThreads_() {
      this.threadsByLinuxPid = {};
      this.threadsByXnid = {};
      this.threadsByXnSyncid = {};
      this.model_.getAllThreads().forEach(
          function(thread) {
            this.threadsByLinuxPid[thread.tid] = thread;
            if(thread.xnid) {
              this.threadsByXnid[thread.xnid] = thread;
              if(thread.sync) {
                this.threadsByXnSyncid[thread.sync] = thread;
              }}
          }.bind(this));
    },

    /**
     * Builds the timeSlices array on each thread based on our knowledge of what
     * each Cpu is doing.  This is done only for Threads that are
     * already in the model, on the assumption that not having any traced data
     * on a thread means that it is not of interest to the user.
     */

    buildPerThreadCpuSlicesFromCpuState_() {
      const SCHEDULING_STATE = tr.model.SCHEDULING_STATE;
      const COBALT_CORE_STATE = tr.model.COBALT_CORE_STATE;
      const XNTHREAD_BLOCK_BITS = tr.model.XNTHREAD_BLOCK_BITS;

      // Push the cpu slices to the threads that they run on.
      for (const cpuNumber in this.model_.kernel.cpus) {
        const cpu = this.model_.kernel.cpus[cpuNumber];

        for (let i = 0; i < cpu.slices.length; i++) {
          const cpuSlice = cpu.slices[i];

          const thread = this.threadsByLinuxPid[cpuSlice.args.tid];
          if (!thread) continue;

          cpuSlice.threadThatWasRunning = thread;

          if (!thread.tempCpuSlices) {
            thread.tempCpuSlices = [];
          }
          thread.tempCpuSlices.push(cpuSlice);
        }
      }

      for (const i in this.wakeups_) {
        const wakeup = this.wakeups_[i];
        const thread = this.threadsByLinuxPid[wakeup.tid];
        if (!thread) continue;
        thread.tempWakeups = thread.tempWakeups || [];
        thread.tempWakeups.push(wakeup);
      }
      for (const i in this.blockedReasons_) {
        const reason = this.blockedReasons_[i];
        const thread = this.threadsByLinuxPid[reason.tid];
        if (!thread) continue;
        thread.tempBlockedReasons = thread.tempBlockedReasons || [];
        thread.tempBlockedReasons.push(reason);
      }

      for (const i in this.gohard_) {
        const xnwshadow = this.gohard_[i];
        const xnthread = this.threadsByXnid[xnwshadow.xnid];
        if (!xnthread) continue;
        xnthread.tempXnwShadows = xnthread.tempXnwShadows || [];
        xnthread.tempXnwShadows.push(xnwshadow);
      }

      for (const i in this.hardened_) {
        const xnhardened = this.hardened_[i];
        const xnthread = this.threadsByXnid[xnhardened.xnid];
        if (!xnthread) continue;
        xnthread.tempXnHarden = xnthread.tempXnHarden || [];
        xnthread.tempXnHarden.push(xnhardened);
      }

      for (const i in this.gorelax_) {
        const xnreason = this.gorelax_[i];
        const xnthread = this.threadsByXnid[xnreason.xnid];
        if (!xnthread) continue;
        xnthread.tempXnReasons = xnthread.tempXnReasons || [];
        xnthread.tempXnReasons.push(xnreason);
      }

      for (const i in this.relaxed_) {
        const xnrelaxed = this.relaxed_[i];
        const xnthread = this.threadsByXnid[xnrelaxed.xnid];
        if (!xnthread) continue;
        xnthread.tempXnRelax = xnthread.tempXnRelax || [];
        xnthread.tempXnRelax.push(xnrelaxed);
      }

      for (const i in this.xnSync_) {
        const syncwakeup = this.xnSync_[i];
        const xnthread = this.threadsByXnSyncid[syncwakeup.xnsyncid];        
        if (!xnthread) continue;
        xnthread.tempXnSyncups = xnthread.tempXnSyncups || [];
        xnthread.tempXnSyncups.push(syncwakeup);
      }

      for (const i in this.sleepons_) {
        const syncsleepon = this.sleepons_[i];
        const xnthread = this.threadsByXnSyncid[syncsleepon.xnsyncid];
        if (!xnthread) continue;
        xnthread.tempXnSyncsleeps = xnthread.tempXnSyncsleeps || [];
        xnthread.tempXnSyncsleeps.push(syncsleepon);
      }

      // Create slices for when the thread is not running.
      this.model_.getAllThreads().forEach(function(thread) {
        if (thread.tempCpuSlices === undefined) return;
        const origSlices = thread.tempCpuSlices;
        delete thread.tempCpuSlices;

        origSlices.sort(function(x, y) {
          return x.start - y.start;
        });

        const wakeups = thread.tempWakeups || [];
        delete thread.tempWakeups;
        wakeups.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const reasons = thread.tempBlockedReasons || [];
        delete thread.tempBlockedReasons;
        reasons.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnsyncups = thread.tempXnSyncups || [];
        delete thread.tempXnSyncups;
        xnsyncups.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnsyncsleeps = thread.tempXnSyncsleeps || [];
        delete thread.tempXnSyncsleeps;
        xnsyncsleeps.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnshadows = thread.tempXnwShadows || [];
        delete thread.tempXnwShadows;
        xnshadows.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnRelaxReasons = thread.tempXnReasons || [];
        delete thread.tempXnReasons;
        xnRelaxReasons.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnharden = thread.tempXnHarden || [];
        delete thread.tempXnHarden;
        xnharden.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xnrelaxed = thread.tempXnRelax || [];
        delete thread.tempXnRelax;
        xnrelaxed.sort(function(x, y) {
          return x.ts - y.ts;
        });

        // Walk the slice list and put slices between each original slice to
        // show when the thread isn't running.
        const slices = [];
        let shadow = undefined;
        let wakeup = undefined;
        let hardened = undefined;        
        let relaxed = undefined;
        let gorelax = undefined;
        let wakeupDuration = 0;
        let wakeupTS = undefined;
        let wakeupState = undefined;
        let runDuration = 0;
        let runTS = undefined;
        let state = undefined;
        let args = {};

        if (origSlices.length) {
          const slice = origSlices[0];

        if (!thread.xnid) { 

          args = { 'name' : thread.name, 'tid' : thread.tid };

          if (wakeups.length && wakeups[0].ts < slice.start) {
            wakeup = wakeups.shift();
            wakeupTS =  wakeup.ts;
            wakeupDuration = slice.start - wakeup.ts;
            args = { 'name' : thread.name, 'tid' : thread.tid, 'prio': wakeup.prio, 'wakeup from tid': wakeup.fromTid};

            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.RUNNABLE, '',
                wakeupTS, args, wakeupDuration));
          } else {
              wakeupDuration = 0;
              wakeupTS = slice.start;
          }

          const sliceTask = new tr.model.ThreadSlice('', thread.name,
              ColorScheme.getColorIdForGeneralPurposeString(thread.name),
              wakeupTS,
              args,
              wakeupDuration + slice.duration, slice.start, slice.duration );
          thread.sliceGroup.pushSlice(sliceTask);

          const runningSlice = new tr.model.ThreadTimeSlice(
              thread, SCHEDULING_STATE.RUNNING, '',
              slice.start, args, slice.duration);
          runningSlice.cpuOnWhichThreadWasRunning = slice.cpu;
          slices.push(runningSlice);

        } else {

          args = { 'name' : thread.name, 'xnid' : thread.xnid };

          if (xnshadows.length && xnshadows[0].ts < slice.start) {
            if (xnharden.length && xnharden[0].ts < slice.end) {
              // Cobalt Shadow thread with gohard for rt scheduling 
              shadow = xnshadows.shift();
              hardened = xnharden.shift();

              wakeupTS =  shadow.ts;
              wakeupDuration = slice.start - shadow.ts;

              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'shadowed from [ROOT]/tid': shadow.fromTid, 'xnstate': tr.model.formatXnThreadState(shadow.xnstate), 'xninfo': shadow.xninfo};

              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNREADY, '',
                  wakeupTS, args, wakeupDuration));

              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'xnstate': tr.model.formatXnThreadState(hardened.xnstate)};
              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNRELAX, '',
                  slice.start, args, hardened.ts - slice.start));

              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'hardened from [ROOT]/tid': hardened.fromTid, 'xnstate': tr.model.formatXnThreadState(hardened.xnstate), 'xninfo': hardened.xninfo};
              runDuration = slice.end - hardened.ts;
              runTS = hardened.ts;

            } else {
              // Cobalt Shadow thread without gohard (ie. weak) for non-rt scheduling 
              shadow = xnshadows.shift();

              wakeupTS =  shadow.ts;
              wakeupDuration = slice.start - shadow.ts;
              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'shadowed from [ROOT]/tid': shadow.fromTid, 'xnstate': tr.model.formatXnThreadState(shadow.xnstate), 'xninfo': shadow.xninfo};

              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNREADY, '',
                  wakeupTS, args, wakeupDuration));

              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNRELAX, '',
                  slice.start, args, hardened.ts - slice.start));

              runDuration = slice.duration;
              runTS = slice.start;
            }

          } else if (xnsyncups.length && xnsyncups[0].ts < slice.start) {
/*
            if (xnRelaxReasons.length && xnRelaxReasons[0].ts < slice.end) {
              // Cobalt Shadow thread with gohard for rt scheduling 
              wakeup = xnsyncups.shift();
              gorelax = xnRelaxReasons.shift();

              wakeupTS =  wakeup.ts;
              wakeupDuration = slice.start - wakeup.ts;              
              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'woken-up by xsyncid': wakeup.xnsyncid, 'xnstate': 'R'};

              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNREADY, '',
                  wakeupTS, args, wakeupDuration));

              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.XNUSER, '',
                  slice.start, args, gorelax.ts - slice.start));

              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'relax to [ROOT]/tid': gorelax.fromTid, 'Reason': gorelax.xnreason};                  
              runDuration = slice.end - gorelax.ts;
              runTS = gorelax.ts;

            } else {
*/              
              wakeup = xnsyncups.shift();

              wakeupTS =  wakeup.ts;
              wakeupDuration = slice.start - wakeup.ts;              
              args = { 'name' : thread.name, 'xnid' : thread.xnid, 'woken-up by xsyncid': wakeup.xnsyncid, 'xnstate': 'R'};

              slices.push(new tr.model.xnThreadTimeSlice(
                thread, COBALT_CORE_STATE.XNREADY, '',
                wakeupTS, args, wakeupDuration));

              runDuration = slice.duration;
              runTS = slice.start;
//            }
          } else {
                wakeupDuration = 0;
                wakeupTS = slice.start;
                runDuration = slice.duration;
                runTS = slice.start;
          }

          const sliceTask = new tr.model.ThreadSlice('', thread.name,
                ColorScheme.getColorIdForGeneralPurposeString(thread.name),
                wakeupTS,
                args,
                wakeupDuration + slice.duration, slice.start, slice.duration );
            thread.sliceGroup.pushSlice(sliceTask);

          const runningSlice = new tr.model.xnThreadTimeSlice(
                thread, COBALT_CORE_STATE.XNUSER, '',
                runTS, args, runDuration);
            runningSlice.cpuOnWhichThreadWasRunning = slice.cpu;
            slices.push(runningSlice);
         }
	}

        if(!thread.xnid) {

        for (let i = 1; i < origSlices.length; i++) {
          let wakeup = undefined;
          let wakeupTS = undefined;
          let wakeupDuration = 0;
          let wakeupState = undefined;

          const prevSlice = origSlices[i - 1];
          const nextSlice = origSlices[i];
          wakeupTS =  nextSlice.start;
          wakeup = undefined;
          wakeupDuration = 0;
          wakeupState = undefined;
          let midDuration = nextSlice.start - prevSlice.end;
          while (wakeups.length && wakeups[0].ts < nextSlice.start) {
            const w = wakeups.shift();
            if (wakeup === undefined && w.ts > prevSlice.end) {
              wakeup = w;
            }
          }
          let blockedReason = undefined;
          while (reasons.length && reasons[0].ts < prevSlice.end) {
            const r = reasons.shift();
          }
          if (wakeup !== undefined &&
              reasons.length &&
              reasons[0].ts < wakeup.ts) {
            blockedReason = reasons.shift();
          }

          // Push a sleep slice onto the slices list, interrupting it with a
          // wakeup if appropriate.
          //const pushSleep = function(state, uThreadGroup) {
            const pushSleep = function(state) {
            
            if (wakeup !== undefined) {
              midDuration = wakeup.ts - prevSlice.end;
            }

            if (blockedReason !== undefined) {
              args = { 'name' : thread.name, 'tid' : thread.tid,
                'kernel callsite when blocked:': blockedReason.caller
              };
              if (blockedReason.iowait) {
                switch (state) {
                  case SCHEDULING_STATE.UNINTR_SLEEP:
                    state = SCHEDULING_STATE.UNINTR_SLEEP_IO;
                    break;
                  case SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL:
                    state = SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL_IO;
                    break;
                  case SCHEDULING_STATE.UNINTR_SLEEP_WAKING:
                    state = SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL_IO;
                    break;
                  default:
                }
              }
              slices.push(new tr.model.ThreadTimeSlice(
                  thread,
                  state, '', prevSlice.end, args, midDuration));
            } else {
              slices.push(new tr.model.ThreadTimeSlice(
                  thread,
                  state, '', prevSlice.end, args, midDuration));
            }
            if (wakeup !== undefined) {
              wakeupDuration = nextSlice.start - wakeup.ts;
              wakeupTS = wakeup.ts;
              args = { 'name' : thread.name, 'tid' : thread.tid, 'prio': wakeup.prio, 'wakeup from tid': wakeup.fromTid};
              slices.push(new tr.model.ThreadTimeSlice(
                  thread, SCHEDULING_STATE.RUNNABLE, '',
                  wakeupTS, args, wakeupDuration));
              wakeup = undefined;
            } else {
              wakeupDuration = 0;
              wakeupTS = nextSlice.start;
            }
          };

          if (prevSlice.args.stateWhenDescheduled === 'S' ||
              prevSlice.args.stateWhenDescheduled === 'I' ||
              prevSlice.args.stateWhenDescheduled === 'P') {
            pushSleep(SCHEDULING_STATE.SLEEPING);
          } else if (prevSlice.args.stateWhenDescheduled === 'S|0x1000') {
            pushSleep(SCHEDULING_STATE.SLEEPING_XNSHADOW);
          } else if (prevSlice.args.stateWhenDescheduled === 'R' ||
                     prevSlice.args.stateWhenDescheduled === 'R+') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.RUNNABLE, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'D') {
            pushSleep(SCHEDULING_STATE.UNINTR_SLEEP);
          } else if (prevSlice.args.stateWhenDescheduled === 'T') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.STOPPED, '',
                prevSlice.end, args , midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 't') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.DEBUG, '',
                prevSlice.end, args , midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'Z') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.ZOMBIE, '',
                prevSlice.end, args , midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'X') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.EXIT_DEAD, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'x') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.TASK_DEAD, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'K') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.WAKE_KILL, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'W') {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.WAKING, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
          } else if (prevSlice.args.stateWhenDescheduled === 'D|K') {
            pushSleep(SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL);
          } else if (prevSlice.args.stateWhenDescheduled === 'D|W') {
            pushSleep(SCHEDULING_STATE.UNINTR_SLEEP_WAKING);
          } else {
            slices.push(new tr.model.ThreadTimeSlice(
                thread, SCHEDULING_STATE.UNKNOWN, '',
                prevSlice.end, args, midDuration));
            wakeupDuration = midDuration;
            wakeupTS = prevSlice.end;
            this.model_.importWarning({
              type: 'parse_error',
              message: 'Unrecognized sleep state: ' +
                  prevSlice.args.stateWhenDescheduled
            });
          }

          const sliceTask = new tr.model.ThreadSlice('', thread.name,
              ColorScheme.getColorIdForGeneralPurposeString(thread.name),
              wakeupTS,
              args,
              wakeupDuration + nextSlice.duration, nextSlice.start, nextSlice.duration );
          thread.sliceGroup.pushSlice(sliceTask);

          const runningSlice = new tr.model.ThreadTimeSlice(
              thread, SCHEDULING_STATE.RUNNING, '',
              nextSlice.start, args, nextSlice.duration);
          runningSlice.cpuOnWhichThreadWasRunning = prevSlice.cpu;
          slices.push(runningSlice);
        }

        } else {

          for (let i = 1; i < origSlices.length; i++) {
            /* time-slicing Xenomai rt-scheduler state*/ 
            shadow = undefined;
            hardened = undefined;            
            wakeup = undefined;
            gorelax = undefined;
            relaxed = undefined;
            wakeupTS = undefined;
            wakeupDuration = 0;
            runTS = undefined;
            runDuration = 0;
            state = undefined;

            const prevSlice = origSlices[i - 1];
            const nextSlice = origSlices[i];

            let midDuration = nextSlice.start - prevSlice.end;

            args = { 'name' : thread.name, 'xnid' : thread.xnid };

            while (xnsyncups.length && xnsyncups[0].ts < nextSlice.start) {
              const s = xnsyncups.shift();
              if (wakeup === undefined && s.ts > prevSlice.end) {
                wakeup = s;
              }
            }

            while (xnshadows.length && xnshadows[0].ts < nextSlice.start) {
              const w = xnshadows.shift();
              if (shadow === undefined && w.ts > prevSlice.end) {
                shadow = w;
              }
            }

            while (xnharden.length && xnharden[0].ts < nextSlice.end) {
              const h = xnharden.shift();
              if (shadow !== undefined && 
                  hardened === undefined && 
                  h.ts > shadow.ts) {
                hardened = h;
              }
            }

            while (xnrelaxed.length && xnrelaxed[0].ts < nextSlice.start) {
              const r = xnrelaxed.shift();
              if (shadow !== undefined &&
                  relaxed === undefined &&              
                  r.ts > prevSlice.end) {
                relaxed = r;
              }
            }

            // Cobalt sheduler Sleep state condition  
            let blockedReason = undefined;
            while (xnsyncsleeps.length && xnsyncsleeps[0].ts < prevSlice.start) {
              const r = xnsyncsleeps.shift();
            }

            if ((wakeup !== undefined &&
                xnsyncsleeps.length &&
                xnsyncsleeps[0].ts < wakeup.ts) || 
                (shadow !== undefined &&
                xnsyncsleeps.length &&
                xnsyncsleeps[0].ts < shadow.ts)) {
              blockedReason = xnsyncsleeps.shift();
            } 

            let relaxedReason = undefined;
            while (xnRelaxReasons.length && xnRelaxReasons[0].ts < prevSlice.start) {
              const r = xnRelaxReasons.shift();
            }

            if (relaxed !== undefined && 
                xnRelaxReasons.length &&
                xnRelaxReasons[0].ts < relaxed.ts) {
              relaxedReason = xnRelaxReasons.shift();
            }

            // Push a sleep slice onto the slices list, interrupting its with a
            // wakeup if appropriate.
            const pushXnSleep = function(state) {
              if (wakeup !== undefined) {
                midDuration = wakeup.ts - prevSlice.end;
              }

              if (shadow !== undefined) {
                midDuration = shadow.ts - prevSlice.end;
              }

              if (blockedReason !== undefined) {
                args = { 'name' : thread.name, 'xnid' : thread.xnid, 'Sleep-on xnsyncid': blockedReason.xnsyncid, 'xnThread going to sleep': blockedReason.xnid};

                state = COBALT_CORE_STATE.XNSUSP;

                slices.push(new tr.model.xnThreadTimeSlice(
                    thread,
                    state, '', prevSlice.end, args, midDuration));

              } else if (relaxedReason !== undefined) {
                if (relaxed !== undefined) {
                  args = { 'name' : thread.name, 'xnid' : thread.xnid, 'Relaxed to [ROOT]/tid': relaxedReason.fromTid, 'Reason': relaxedReason.xnreason, 'xnstate': tr.model.formatXnThreadState(relaxed.xnstate), 'xnstatus': relaxed.xnstatus, 'xninfo': relaxed.xninfo};
                  midDuration = shadow.ts - relaxed.ts;
                } else {
                  args = { 'name' : thread.name, 'xnid' : thread.xnid, 'Relaxed to [ROOT]/tid': relaxedReason.fromTid, 'Reason': relaxedReason.xnreason };                  
                }

                state = COBALT_CORE_STATE.XNRELAX;
/* Todo: Add relaxed reasons
                if (relaxedReason.xnstatus) {
                  switch (state) {
                    case SCHEDULING_STATE.UNINTR_SLEEP:
                      state = SCHEDULING_STATE.UNINTR_SLEEP_IO;
                      break;
                    case SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL:
                      state = SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL_IO;
                      break;
                    case SCHEDULING_STATE.UNINTR_SLEEP_WAKING:
                      state = SCHEDULING_STATE.UNINTR_SLEEP_WAKE_KILL_IO;
                      break;
                    default:
                  }
                }
*/
                slices.push(new tr.model.xnThreadTimeSlice(
                    thread,
                    state, '', prevSlice.end, args, midDuration));
              } else {
                slices.push(new tr.model.xnThreadTimeSlice(
                    thread,
                    state, '', prevSlice.end, args, midDuration));
              }

              if (shadow !== undefined) {
                if (hardened !== undefined) {

                  wakeupTS = shadow.ts;
                  wakeupDuration = nextSlice.start - shadow.ts;

                  runTS = hardened.ts;
                  runDuration = nextSlice.end - hardened.ts;

                  args = { 'name' : thread.name, 'xnid' : thread.xnid, 'shadowed from [ROOT]/tid': shadow.fromTid, 'xnstate': tr.model.formatXnThreadState(shadow.xnstate), 'xninfo': shadow.xninfo};
                  slices.push(new tr.model.xnThreadTimeSlice(
                      thread, COBALT_CORE_STATE.XNREADY, '',
                      wakeupTS, args, wakeupDuration));

                  slices.push(new tr.model.xnThreadTimeSlice(
                      thread, COBALT_CORE_STATE.XNRELAX, '',
                      nextSlice.start, args, hardened.ts - nextSlice.start));

                  args = { 'name' : thread.name, 'xnid' : thread.xnid, 'hardened from [ROOT]/tid': hardened.fromTid, 'xnstate': tr.model.formatXnThreadState(hardened.xnstated), 'xninfo': shadow.xninfo};
                  shadow = undefined;
                  hardened = undefined;

                } else {

                  wakeupTS = shadow.ts;
                  wakeupDuration = nextSlice.start - shadow.ts;

                  runTS = nextSlice.start;
                  runDuration = nextSlice.duration;

                  args = { 'name' : thread.name, 'xnid' : thread.xnid, 'shadowed from [ROOT]/tid': shadow.fromTid, 'xnstate': tr.model.formatXnThreadState(shadow.xnstate), 'xninfo': shadow.xninfo};
                  slices.push(new tr.model.xnThreadTimeSlice(
                      thread, COBALT_CORE_STATE.XNREADY, '',
                      wakeupTS, args, wakeupDuration));
                  shadow = undefined;                  

                }
              } else if (wakeup !== undefined) {
                wakeupDuration = nextSlice.start - wakeup.ts;
                wakeupTS = wakeup.ts;

                runDuration = nextSlice.duration;
                runTS = nextSlice.start;

                args = { 'name' : thread.name, 'xnid' : thread.xnid, 'Woken-up by xnsyncid': wakeup.xnsyncid};
                slices.push(new tr.model.xnThreadTimeSlice(
                    thread, COBALT_CORE_STATE.XNREADY, '',
                    wakeupTS, args, wakeupDuration));

                wakeup = undefined;
              } else {
                wakeupTS = nextSlice.start;
                wakeupDuration = 0;

                runTS = nextSlice.start;
                runDuration = nextSlice.duration;
              }
            };

            if (prevSlice.args.xnStateWhenDescheduled === 'S') { //Forcibly suspended.
              pushXnSleep(COBALT_CORE_STATE.XNSUSP);
              //wakeupDuration = midDuration;
              //wakeupTS = prevSlice.end;
            } else if (prevSlice.args.xnStateWhenDescheduled === 'w' ||
                       prevSlice.args.xnStateWhenDescheduled === 'W') { //Waiting for a resource, with or without timeout.
              pushXnSleep(COBALT_CORE_STATE.XNPEND);
            } else if (prevSlice.args.xnStateWhenDescheduled === 'D') { //Delayed (without any other wait condition).
              pushXnSleep(COBALT_CORE_STATE.XNDELAY);
            } else if (prevSlice.args.xnStateWhenDescheduled === 'R' ) { //Runnable
              slices.push(new tr.model.xnThreadTimeSlice(
                    thread, COBALT_CORE_STATE.XNREADY, '',
                    prevSlice.end, args, midDuration));
              wakeupDuration = midDuration;
              wakeupTS = prevSlice.end;
              runDuration = nextSlice.duration;              
              runTS = nextSlice.start;
            } else if (prevSlice.args.xnStateWhenDescheduled === '-' ||
                       prevSlice.args.xnStateWhenDescheduled === 'U') { //Unstarted or dormant.
              pushXnSleep(COBALT_CORE_STATE.XNDORMANT);
           } else if (prevSlice.args.xnStateWhenDescheduled === 'X') { //Relaxed shadow.
              pushXnSleep(COBALT_CORE_STATE.XNRELAX);
           } else if (prevSlice.args.xnStateWhenDescheduled === 'H') { //Held in emergency
              slices.push(new tr.model.xnThreadTimeSlice(
                    thread, COBALT_CORE_STATE.XNHELD, '',
                    prevSlice.end, args, midDuration));
              wakeupDuration = midDuration;
              wakeupTS = prevSlice.end;
              runDuration = nextSlice.duration;              
              runTS = nextSlice.start;
            } else if (prevSlice.args.xnStateWhenDescheduled === 'b') { //Priority boost undergoing
              slices.push(new tr.model.xnThreadTimeSlice(
                    thread, COBALT_CORE_STATE.XNBOOST, '',
                    prevSlice.end, args, midDuration));
              wakeupDuration = midDuration;
              wakeupTS = prevSlice.end;
              runDuration = nextSlice.duration;              
              runTS = nextSlice.start;
            } else if (prevSlice.args.xnStateWhenDescheduled === 'T') { //Ptraced and stopped.
              slices.push(new tr.model.xnThreadTimeSlice(
                    thread, COBALT_CORE_STATE.XNSSTEP, '',
                    prevSlice.end, args, midDuration));
              wakeupDuration = midDuration;
              wakeupTS = prevSlice.end;
              runDuration = nextSlice.duration;              
              runTS = nextSlice.start;
            } else if (prevSlice.args.xnStateWhenDescheduled === 'l') { //Locks scheduler
            } else if (prevSlice.args.xnStateWhenDescheduled === 'r') { //Undergoes round-robin.
            } else if (prevSlice.args.xnStateWhenDescheduled === 't') { //Runtime mode errors notified.
            } else if (prevSlice.args.xnStateWhenDescheduled === 'L') { //Lock breaks trapped.
              pushXnSleep(COBALT_CORE_STATE.XNTRAPLB);
            } else {
              slices.push(new tr.model.xnThreadTimeSlice(
                  thread, COBALT_CORE_STATE.w, '',
                  prevSlice.end, args, midDuration));
              wakeupDuration = midDuration;
              wakeupTS = prevSlice.end;
              runDuration = nextSlice.duration;              
              runTS = nextSlice.start;
              this.model_.importWarning({
                type: 'parse_error',
                message: 'Unrecognized sleep state: ' +
                    prevSlice.args.xnStateWhenDescheduled
              });
            }
            const sliceTask = new tr.model.ThreadSlice('', thread.name,
                ColorScheme.getColorIdForGeneralPurposeString(thread.name),
                wakeupTS,
                args,
                wakeupDuration + nextSlice.duration, nextSlice.start, nextSlice.duration );
            thread.sliceGroup.pushSlice(sliceTask);

            const runningSlice = new tr.model.xnThreadTimeSlice(
                thread, COBALT_CORE_STATE.XNUSER, '',
                runTS, args, runDuration);
            runningSlice.cpuOnWhichThreadWasRunning = prevSlice.cpu;
            slices.push(runningSlice);
          }
        }
        thread.timeSlices = slices;

      }, this);
    },

    /**
     * Records the fact that a net packet has become queue. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetQueued(ts, adapter, key, pid) {
      this.netqueues_.push({ts, tid: pid, 'adapter': adapter, 'key': key});
    },

    /**
     * Records the fact that a net xmit packet has become done. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetXmit(ts, adapter, key, pid ) {
      this.netxmits_.push({ts, tid: pid, 'adapter': adapter, 'key': key});
    },

    /**
     * Records the fact that a net xmit packet has become timeout. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetTimeout(ts, adapter, queuenum, pid ) {
      this.nettimeouts_.push({ts, tid: pid, 'adapter': adapter, 'queue': queuenum });
    },

    /**
     * Records the fact that a net rx packet has become rx_exit. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetRecv(ts, adapter, queuenum, key, pid) {
      this.netrecvs_.push({ts, tid: pid, 'adapter': adapter, 'queue': queuenum, 'key': key});
    },

    /**
     * Records the fact that a net irq has become scheduled. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetTxIrq(ts, cpunum, pid, duration) {
      this.nettxirqs_.push({ts, tid: pid, 'cpu': cpunum, 'duration': duration});
    },

    /**
     * Records the fact that a net irq has become scheduled. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetRxIrq(ts, cpunum, pid, duration) {
      this.netrxirqs_[pid] = this.netrxirqs_[pid] || [];
      this.netrxirqs_[pid].push({ts, tid: pid, 'cpu': cpunum, 'duration': duration});
    },

    /**
     * Records the fact that a net irq has become scheduled. This data will
     * eventually get used to derive each thread's timeSlices array.
     */
    markNetIrq(ts, cpunum, pid, duration, adapter, queuenum, irq) {
      this.netirqs_.push({ts, tid: pid, 'cpu': cpunum, 'duration': duration, 'adapter': adapter, 'queue': queuenum, 'key': irq, });
    },

    /**
     * Builds the timeSlices array on each thread based on our knowledge of what
     * each Network devices are doing.  This is done only for Threads that are
     * already in the model, on the assumption that not having any traced data
     * on a thread means that it is not of interest to the user.
     */

    buildPerThreadNetSlicesFromEthScope_() {
      const NETWORKING_STATE = tr.e.net.NETWORKING_STATE;
      const ethIrqHandlerRE = tr.e.net.ethIrqHandlerRE;
      const vnetIrqHandlerRE = tr.e.net.vnetIrqHandlerRE;

      // Push the Objects snapshots to the threads that opened the AF_SOCKET.
      for (const ethName of Object.keys(this.model_.netdevs)) {
        const instances = this.model_.netdevs[ethName].objects.getAllObjectInstances();

        for (let i = 0; i < instances.length; i++) {
          const snapshots = instances[i].snapshots;

          for (let j = 0; j < snapshots.length; j++) {
            const snapshot = snapshots[j];

            const thread = this.threadsByLinuxPid[snapshot.args.calling_tid];
            if (!thread) continue;

            snapshot.snapshottedOnThread = thread;

            if (!thread.tempNetSnapshots) {
              thread.tempNetSnapshots = [];
            }
            thread.tempNetSnapshots.push(snapshot);

            let irqThread = undefined;
            if (!thread.tempNetRxIrqs) thread.tempNetRxIrqs = {};
            if (!thread.tempNetTxIrqs) thread.tempNetTxIrqs = [];
            if (!thread.tempNetTimeouts) thread.tempNetTimeouts = {};
            if (!thread.tempNetTimeouts[ethName]) thread.tempNetTimeouts[ethName] = [];
            if (!thread.tempNetIrqs) thread.tempNetIrqs = {};
            if (!thread.tempNetIrqs[ethName]) thread.tempNetIrqs[ethName] = [];
            if (!thread.tempNetIrqs[ethName][snapshot.objectInstance.scopedId.queue]) {

              thread.tempNetIrqs[ethName][snapshot.objectInstance.scopedId.queue] = [];
              thread.tempNetTimeouts[ethName][snapshot.objectInstance.scopedId.queue] = [];

              // list all IRQ events for thread related to Ethernet hw-queues queues
              for (const l in this.netirqs_) {
                const irqed = this.netirqs_[l];
                if ( irqed.adapter !== undefined && irqed.queue !== undefined ) {
                  if ( ethName !==  irqed.adapter ||
                       snapshot.objectInstance.scopedId.queue !== irqed.queue ) continue;
                  // Add ethernet queue specific irq handler event
                  thread.tempNetIrqs[ethName][snapshot.objectInstance.scopedId.queue].push(irqed);
                  if (!irqThread &&
                      this.threadsByIrqs[irqed.key] &&
                      this.threadsByIrqs[irqed.key].length)
                    irqThread = this.threadsByIrqs[irqed.key][0];
                }
              }
              // list all Timeout events for each thread related to Ethernet hw-queues
              for (const t in this.nettimeouts_) {
                const timedout = this.nettimeouts_[t];
                if ( timedout.adapter !== undefined && timedout.queue !== undefined ) {
                  if ( ethName !==  timedout.adapter ||
                       snapshot.objectInstance.scopedId.queue !== timedout.queue ) continue;
                    // Add ethernet queue specific Timeout event
                    thread.tempNetTimeouts[ethName][snapshot.objectInstance.scopedId.queue].push(timedout);
                }
              }
            }
            /*
            */
            for (const cpuNumber in this.model_.kernel.cpus) {
              // Add NET_TX events from any CPU
              if (!thread.tempNetTxIrqs[cpuNumber]) {

                thread.tempNetTxIrqs[cpuNumber] = [];
                for (const h in this.nettxirqs_) {
                  const txirqed = this.nettxirqs_[h];
                  if ( txirqed.cpu === parseInt(cpuNumber))
                    thread.tempNetTxIrqs[cpuNumber].push(txirqed);
                }
              }
            }

            if ( irqThread &&
                 this.netrxirqs_[irqThread.thread.tid]) {
              /* Add ONLY once all ETH queue related threaded irq NET_RX event subset
               */
              if (!thread.tempNetRxIrqs[irqThread.thread.tid])
                thread.tempNetRxIrqs[irqThread.thread.tid] = this.netrxirqs_[irqThread.thread.tid];
            }

            if (this.netrxirqs_[thread.tid]) {
              /* Add user-thread related NET_RX event subset
               */
              if (!thread.tempNetRxIrqs[thread.tid])
                thread.tempNetRxIrqs[thread.tid] = this.netrxirqs_[thread.tid];
            }
          }
        }
      }

      for (const i in this.netqueues_) {
        const queued = this.netqueues_[i];
        const thread = this.threadsByLinuxPid[queued.tid];
        if (!thread) continue;
        thread.tempNetQueues = thread.tempNetQueues || [];
        thread.tempNetQueues.push(queued);
      }

      for (const i in this.netxmits_) {
        const xmited = this.netxmits_[i];
        const thread = this.threadsByLinuxPid[xmited.tid];
        if (!thread) continue;
        thread.tempNetXmits = thread.tempNetXmits || [];
        thread.tempNetXmits.push(xmited);
      }

      for (const i in this.netrecvs_) {
        const recved = this.netrecvs_[i];
        const thread = this.threadsByLinuxPid[recved.tid];
        if (!thread) continue;
        thread.tempNetRecvs = thread.tempNetRecvs || [];
        thread.tempNetRecvs.push(recved);
      }

      this.model_.getAllThreads().forEach(function(thread) {
        if (thread.tempNetSnapshots === undefined) return;
        const origSnapshots = thread.tempNetSnapshots;
        delete thread.tempNetSnapshots;

        origSnapshots.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const queues = thread.tempNetQueues || [];
        delete thread.tempNetQueues;
        queues.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const xmits = thread.tempNetXmits || [];
        delete thread.tempNetXmits;
        xmits.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const recvs = thread.tempNetRecvs || [];
        delete thread.tempNetRecvs;
        recvs.sort(function(x, y) {
          return x.ts - y.ts;
        });

        const txirqs = thread.tempNetTxIrqs || [];
        delete thread.tempNetTxIrqs;
        for (const cpuNumber of Object.keys(txirqs)) {
          txirqs[cpuNumber].sort(function(x, y) {
            return x.ts - y.ts;
          });
        }

        const rxirqs = thread.tempNetRxIrqs || {};
        delete thread.tempNetRxIrqs;
        for (const tid of Object.keys(rxirqs)) {
          rxirqs[tid].sort(function(x, y) {
            return x.ts - y.ts;
          });
        }

        const irqs = thread.tempNetIrqs || {};
        delete thread.tempNetIrqs;
        for (const caller of Object.keys(irqs)) {
          for (const idx of Object.keys(irqs[caller])) {
            irqs[caller][idx].sort(function(x, y) {
              return x.ts - y.ts;
            });
          }
        }

        const timeouts = thread.tempNetTimeouts || {};
        delete thread.tempNetTimeouts;
        for (const netif of Object.keys(timeouts)) {
          for (const qidx of Object.keys(timeouts[netif])) {
            timeouts[netif][qidx].sort(function(x, y) {
              return x.ts - y.ts;
            });
          }
        }

        // Walk the slice list and put slices between each original slice to
        // show when the thread isn't running.
        /*
        NETWORKING_STATE.QUEUED
        NETWORKING_STATE.XMIT
        NETWORKING_STATE.TIMEOUT
        NETWORKING_STATE.RECV
        NETWORKING_STATE.IDLE
        NETWORKING_STATE.STOPPED
        NETWORKING_STATE.UNKNOWN
        */
        let duration = 0.001; //1us as default time budget
        let queueDuration = 0;
        let recvDuration = 0;
        let schedDuration = 0;
        let xmitDuration = 0;

        let queueTS = undefined;
        let recvTS = undefined;
        let schedTS = undefined;
        let xmitTS = undefined;

        let state = undefined;
        let args = {};
        const aslices = [];
        let aslice = undefined;
        let asubslice = undefined;
        let label = 'unknown'

        let queue = undefined;
        let recv = undefined;
        let xmit = undefined;
        let timeout = undefined;

        if (origSnapshots.length) {

          const snapshot = origSnapshots[0];

          while (queues.length && queues[0].ts < snapshot.ts) {
            const q = queues.shift();
            if (q.key === snapshot.args.ref ) {
              queue = q;
            }
          }

          while (recvs.length && recvs[0].ts < snapshot.ts) {
            const r = recvs.shift();
            if (r.key === snapshot.args.ref ) {
              recv = r;
            }
          }

          if ( recv === undefined && rxirqs[thread.tid] ) {
            // fallback on user-thread NET_RX call
            while ( rxirqs[thread.tid].length &&
                    rxirqs[thread.tid][0].ts < snapshot.ts ) {
              const r = rxirqs[thread.tid].shift();
              if (r.tid === snapshot.args.calling_tid) {
                recv = r;
              }
            }
          }

          while (xmits.length && xmits[0].ts < snapshot.ts) {
            const x = xmits.shift();
            if (x.key === snapshot.args.ref ) {
              xmit = x;
            }
          }

          let irq = undefined;
          let rxrirq = undefined;
          while (irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue] &&
                 irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].length &&
                 irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue][0].ts < snapshot.ts) {
            irq = irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].shift();
          }

          while (timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue] &&
                 timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].length &&
                 timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue][0].ts < snapshot.ts) {
            timeout = timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].shift();
          }

          let lastIrq = undefined;
          let lastRxIrq = undefined;
          let lastXmit = undefined;
          let lastRecv = undefined;
          let lastTimeout = undefined;

          if (origSnapshots.length === 1) {

            if (xmits.length)
              lastXmit = xmits.shift();
            if (recvs.length)
              lastRecv = recvs.shift();
            else if (rxirqs[thread.tid] &&
                     rxirqs[thread.tid].length)
              lastRecv = rxirqs[thread.tid].shift();

            if (timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue] && 
                timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].length) {
              lastTimeout = timeouts[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].shift();
            }

            if (irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue] && 
                irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].length) {
              lastIrq = irqs[snapshot.objectInstance.parent.eid][snapshot.objectInstance.scopedId.queue].shift();
              if ( lastIrq &&
                   this.threadsByIrqs[lastIrq.key] &&
                   this.threadsByIrqs[lastIrq.key].length) {
                const irqThread = this.threadsByIrqs[lastIrq.key][0];
                if ( irqThread ) {
                  // ignore all NET_RX calls prior the threaded-irq handler entry 
                  while ( rxirqs[irqThread.thread.tid] &&
                          rxirqs[irqThread.thread.tid].length ) {

                    const r = rxirqs[irqThread.thread.tid].shift();
                    if ( lastRxIrq !== undefined &&
                         r.ts > snapshot.ts &&
                         r.ts > lastIrq.ts ) {
                         lastRxIrq = r;
                         break;
                    }
                  }
                }
              }
            }
          }

          queueTS = snapshot.ts;
          recvTS = snapshot.ts;
          schedTS = snapshot.ts;
          xmitTS = snapshot.ts;

          args = { 'key' : snapshot.args.ref,
                   'netcall' : snapshot.args.calling_fct,
                   'queuedby': undefined,
                 };

          switch (snapshot.args.type) {

            case 'NET_TX':
              label = 'egress'
              state = NETWORKING_STATE.TXQUEUED;

              if (queue) {
                queueTS =  queue.ts;
                queueDuration = snapshot.ts - queueTS;
                duration = queueDuration > 0 ? queueDuration :  duration ;
                args['queuedby'] = queue.tid;
              }

              if (origSnapshots.length === 1) {
                // snapshot[0] is the 1st & the last

                if (lastXmit) {
                  xmitTS = lastXmit.ts;
                  schedDuration = lastXmit.ts - schedTS ;
                }

                if (lastIrq) {
                  // NET_RX call from threaded-irq handler
                  xmitDuration = (lastIrq.ts + lastIrq.duration) - xmitTS;
                }

                if (lastRxIrq) {
                  // NET_RX call from threaded-irq handler
                  //xmitDuration = (lastRxIrq.ts + lastRxIrq.duration) - xmitTS;
                  if (this.threadsByLinuxPid[lastRxIrq.tid])
                    args['netsched'] = this.threadsByLinuxPid[lastRxIrq.tid].name;
                }
              }
              break;

            case 'NET_RX':
              label = 'ingress'
              state = NETWORKING_STATE.RXQUEUED;

              if (irq) {
                if (this.threadsByIrqs[irq.key] &&
                  this.threadsByIrqs[irq.key].length) {
                  const irqThread = this.threadsByIrqs[irq.key][0];
                  if ( irqThread ) {
                    args['queuedby'] = irqThread.thread.tid;
                    args['irqname'] = irqThread.name;
                    args['irqnum'] = irqThread.irq;
                  }
                }
              }

              if ( origSnapshots.length === 1) {
                // snapshot[0] is the 1st & the last

                if ( lastRecv )  {
                  // netif_rx_exit or NET_TX direct call from thread
                  recvTS = lastRecv.ts + lastRecv.duration;
                  recvDuration = recvTS - snapshot.ts;
                } else if ( irq && lastRxIrq) {
                  // NET_RX exit call from a threaded-irq handler
                  recvTS = lastRxIrq.ts + lastRxIrq.duration;
                  recvDuration = recvTS - snapshot.ts;
                }
              }
              break;

            case 'XDP_TX':
              label = 'egress'
              duration = 0;
              break;

            case 'XDP_RX':
              label = 'ingress'
              duration = 0;
              break;

            default:
              break;
          }

          aslice = new tr.model.AsyncSlice(tr.e.net.EthertypeToHuman(snapshot.args.protocol),
             label,
             tr.e.net.getColorForNet(snapshot.args.protocol),
             queueTS,
             snapshot.args,
             duration);
          aslice.important = false;

          if (queueDuration > 0) {
             // AsyncSlice's to show when a RX or TX packet is queued :
             asubslice = new tr.e.net.EthAsyncSlice(
                 state, '',
                 queueTS, args, queueDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration ;
          }

          if (xmitDuration > 0 ) {
            // AsyncSlice's to show when a TX packet is xmited :
            asubslice = new tr.e.net.EthAsyncSlice(
                NETWORKING_STATE.XMIT, '',
                xmitTS, args, xmitDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration + xmitDuration;
          }

          if (recvDuration > 0 ) {
             // AsyncSlice's to show when a  RX packet is recved :
            aslice.duration = queueDuration + recvDuration;
            asubslice = new tr.e.net.EthAsyncSlice(
                 NETWORKING_STATE.RECV, '',
                schedTS, args, recvDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration + xmitDuration;
          }
          aslices.push(aslice);

        }

        for (let j = 1; j < origSnapshots.length; j++) {

          const prevSnapshot = origSnapshots[j - 1];
          const nextSnapshot = origSnapshots[j];

          queue = undefined;
          recv = undefined;
          xmit = undefined;
          timeout = undefined;

          while (queues.length && queues[0].ts < nextSnapshot.ts) {
            const q = queues.shift();

            if (q.ts > prevSnapshot.ts &&
                q.key === nextSnapshot.args.ref )
              queue = q;
          }

          while (recvs.length && recvs[0].ts < nextSnapshot.ts) {
            const r = recvs.shift();

            if (recv === undefined &&
                r.ts > prevSnapshot.ts &&
              r.key === prevSnapshot.args.ref )
              recv = r;
          }

          if (!recv && rxirqs[thread.tid]) {
            // fallback on thread NET_RX call
            while (rxirqs[thread.tid] &&
                   rxirqs[thread.tid].length &&
                   rxirqs[thread.tid][0].ts < nextSnapshot.ts) {
              const r = rxirqs[thread.tid].shift();

              if (recv === undefined &&
                  r.ts > prevSnapshot.ts &&
                  r.tid === prevSnapshot.args.calling_tid)
                recv = r;
            }
          }

          while (xmits.length && xmits[0].ts < nextSnapshot.ts) {
            const x = xmits.shift();

            if (xmit === undefined &&
                x.ts > prevSnapshot.ts &&
                x.key === prevSnapshot.args.ref)
              xmit = x;
          }

          let prevIrq = undefined;
          let nextIrq = undefined;
          let prevTxIrq = undefined;
          let prevRxIrq = undefined;
          let nextRxIrq = undefined;

          /* process snapshot[n-1] event calls:
           * - any thread call to NET_TX & NET_RX functions
           * - any Ethernet top-half irq & bottom-halt thread call to NET_TX & NET_RX functions 
           * - any net_dev_xmit entry call
           * - any netif_rx or netif_rx_ni exit call
            (1us default ingress/egress injection-time budget)
          */

          // Locate the first Timeout event starting right BEFORE the next Ethernet snapshot frames got queued
          while (timeouts[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue] && 
                 timeouts[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue].length &&
                   timeouts[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue][0].ts < nextSnapshot.ts) {
            const t = timeouts[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue].shift();

            if ( t !== undefined &&
                 t.ts > prevSnapshot.ts )
              timeout = t;
          }

          // Locate the irq-handler starting right AFTER the previous EThernet snapshot frames got queued
          while (irqs[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue] &&
                 irqs[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue].length &&
                 irqs[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue][0].ts < nextSnapshot.ts) {

            const i = irqs[prevSnapshot.objectInstance.parent.eid][prevSnapshot.objectInstance.scopedId.queue].shift();

            if ( i !== undefined &&
                 i.ts > prevSnapshot.ts ) {
              prevIrq = i;
              // Locate all NET_TX events prior the irq-handler start
              for (const cpuNumber of Object.keys(txirqs)) {
                while (txirqs[cpuNumber].length &&
                       txirqs[cpuNumber][0].ts < prevIrq.ts) {
                  prevTxIrq = txirqs[cpuNumber].shift();
                }
                // Has an NET_TX handler occured between xmit_start and NET_RX
                if (prevTxIrq !== undefined) {
                  if (prevTxIrq.ts > prevSnapshot.ts) break;
                  else prevTxIrq = undefined;
                }
              }
              break;
            }
          }

          if (prevIrq) {
            // Locate the NET_RX event right after threaded-irq handler
            if (this.threadsByIrqs[prevIrq.key] &&
                this.threadsByIrqs[prevIrq.key].length) {
              const irqThread = this.threadsByIrqs[prevIrq.key][0];
              if ( irqThread ) {
                while (rxirqs[irqThread.thread.tid] &&
                       rxirqs[irqThread.thread.tid].length &&
                       rxirqs[irqThread.thread.tid][0].ts < nextSnapshot.ts ) {

                  const r = rxirqs[irqThread.thread.tid].shift();
                  if (prevRxIrq === undefined &&
                      r.ts > prevSnapshot.ts &&
                      r.ts > prevIrq.ts )
                    prevRxIrq = r;
                    recv = prevRxIrq;
                }
              }
            }
          }

          duration = 0.001;
          queueDuration = 0;
          recvDuration = 0;
          schedDuration = 0;
          xmitDuration = 0;

          xmitTS = prevSnapshot.ts;
          recvTS = prevSnapshot.ts;
          schedTS = prevSnapshot.ts;

          const midDuration = nextSnapshot.ts - prevSnapshot.ts;
          if (midDuration) {
            // the two snapshots don't have the same timestamp

            aslice = aslices.pop();

            asubslice = aslice.subSlices.pop();
            if (asubslice)
              args = asubslice.args;
            else
              args = { 'key' : prevSnapshot.args.ref,
                       'netcall' : prevSnapshot.args.calling_fct,
                       'queuedby' : undefined,
                     };

            switch (prevSnapshot.args.type) {

              case 'NET_TX':
                state = NETWORKING_STATE.IDLE;

                if (xmit) {
                  xmitTS = xmit.ts;
                  schedDuration = xmitTS - schedTS;
                }
                if (prevTxIrq) {
                  xmitTS = (prevTxIrq.ts + prevTxIrq.duration);
                  schedDuration = xmitTS - schedTS;
                  if (this.threadsByLinuxPid[prevTxIrq.tid]) {
                    const irqThread = this.threadsByLinuxPid[prevTxIrq.tid];
                    if ( irqThread )
                      args['netsched'] = irqThread.name;
                  }
                }
                if (prevIrq) {
                  xmitDuration = (prevIrq.ts + prevIrq.duration) - xmitTS;
                  if (this.threadsByIrqs[prevIrq.key] &&
                      this.threadsByIrqs[prevIrq.key].length) {
                    const irqThread = this.threadsByIrqs[prevIrq.key][0];
                    if ( irqThread ) {
                      args['irqname'] = irqThread.name;
                      args['irqnum'] = irqThread.irq;
                    }
                  }
                }

                if ( aslice ) {

                    if (asubslice &&
                        xmit &&
                        args.queuedby === xmit.tid ){
                      aslice.subSlices.push(asubslice);
                      aslice.duration += xmitDuration > 0 ? schedDuration + xmitDuration : duration;
                    } else if (asubslice &&
                             prevIrq  &&
                             this.threadsByIrqs[prevIrq.key] &&
                             this.threadsByIrqs[prevIrq.key].length ) {
                      aslice.subSlices.push(asubslice);
                      aslice.duration += xmitDuration > 0 ? schedDuration + xmitDuration : duration;
                    } else {
                      // New thread NET_RX call
                      aslice.start = schedTS;
                      aslice.duration = xmitDuration > 0 ? schedDuration + xmitDuration : duration;
                    }
                }
                break;

              case 'NET_RX':
                state = NETWORKING_STATE.IDLE;

                if (recv) {
                  recvTS = recv.ts + recv.duration;
                  recvDuration = recvTS - schedTS;
                }

                if ( aslice ) {
                  if (asubslice && recv &&
                      args.queuedby === recv.tid) {
                    // Old thread NET_RX called by prior handled rx-irq
                    aslice.subSlices.push(asubslice);
                    aslice.duration += recvDuration > 0 ? recvDuration : duration;
                  }
                  else {
                  // New thread NET_RX call, ignore prior handled rx-irq
                    aslice.start = schedTS;
                    aslice.duration = recvDuration > 0 ? recvDuration : duration;
                  }
                }
                break;

              case 'XDP_TX':
                state = NETWORKING_STATE.WAIT;
                if (timeout) {
                  schedTS = timeout.ts;
                  schedDuration = nextSnapshot.ts - schedTS;
                  xmitDuration = schedTS - prevSnapshot.ts;
                }

                if ( aslice ) {

                    if (asubslice &&
                        xmit &&
                        args.queuedby === xmit.tid ) {
                      aslice.subSlices.push(asubslice);
                    } else if (asubslice &&
                             prevIrq  &&
                             this.threadsByIrqs[prevIrq.key] &&
                             this.threadsByIrqs[prevIrq.key].length ) {
                      aslice.subSlices.push(asubslice);
                    }
                    aslice.duration = midDuration;
                }
                break;

              case 'XDP_RX':
                state = NETWORKING_STATE.WAIT;

                if (timeout) {
                  schedTS = timeout.ts;
                  schedDuration = nextSnapshot.ts - schedTS;
                  recvDuration = schedTS - prevSnapshot.ts;
                }

                if ( aslice ) {

                  if (asubslice && recv &&
                      args.queuedby === recv.tid) {
                    // Old thread NET_RX called by prior handled rx-irq
                    aslice.subSlices.push(asubslice);
                  }
                  aslice.duration = midDuration;
                }
                break;

              default:
                break;
            }

            if (schedDuration > 0 && aslice ) {
              // AsyncSlice's to show when the TX packets is scheduled:
              asubslice = new tr.e.net.EthAsyncSlice(
                  state, '',
                  schedTS, args, schedDuration);
              aslice.subSlices.push(asubslice);
            }

            if (xmitDuration > 0 && aslice ) {
              // AsyncSlice's to show when the TX packets is xmited :
              asubslice = new tr.e.net.EthAsyncSlice(
                  NETWORKING_STATE.XMIT, '',
                  xmitTS, args, xmitDuration);
              aslice.subSlices.push(asubslice);
            }

            if (recvDuration > 0 && aslice ) {
               // AsyncSlice's to show when the TX packets is recved :
              asubslice = new tr.e.net.EthAsyncSlice(
                   NETWORKING_STATE.RECV, '',
                  schedTS, args, recvDuration);
              aslice.subSlices.push(asubslice);
            }

            if (aslice !== undefined )
              aslices.push(aslice);
          }

          /* process snapshot[n] event calls:
           * - any thread call to NET_TX & NET_RX functions
           * - any Ethernet top-half irq & bottom-halt thread call to NET_TX & NET_RX functions 
           * - any net_dev_queues entry call
           *
           * process snapshot[last] event calls:
           * - any net_dev_xmit and net_dev_queues entry call
           * - any netif_rx or netif_rx_ni exit call
            (1us default ingress/egress injection-time budget)
          */
          if (nextSnapshot.objectInstance.parent.eid === prevSnapshot.objectInstance.parent.eid &&
              nextSnapshot.objectInstance.scopedId.queue === prevSnapshot.objectInstance.scopedId.queue) {

            nextIrq = prevIrq;
            nextRxIrq = prevRxIrq;

            }

          // Locate the irq-handler event starting right BEFORE the next Ethernet snapshot frames got queued
          while (irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue] && 
                 irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue].length &&
                   irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue][0].ts < nextSnapshot.ts) {
            const i = irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue].shift();

            if ( i !== undefined &&
                 i.ts > prevSnapshot.ts )
              nextIrq = i;
          }

          lastIrq = undefined;
          lastRxIrq = undefined;
          lastXmit = undefined;
          lastRecv = undefined;
          lastTimeout = undefined;
          if (j === origSnapshots.length - 1 ) {
            // The next snapshot is the last on the snapshots list
            if (xmits.length)
              lastXmit = xmits.shift();
            if (recvs.length)
              lastRecv = recvs.shift();
            else if ( rxirqs[thread.tid] &&
                      rxirqs[thread.tid].length)
              lastRecv = rxirqs[thread.tid].shift();

            if (irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue] &&
                irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue].length) {
              lastIrq = irqs[nextSnapshot.objectInstance.parent.eid][nextSnapshot.objectInstance.scopedId.queue].shift();
              if ( lastIrq &&
                   this.threadsByIrqs[lastIrq.key] &&
                   this.threadsByIrqs[lastIrq.key].length) {
                const irqThread = this.threadsByIrqs[lastIrq.key][0];
                if ( irqThread ) {
                  // ignore all NET_RX calls prior the threaded-irq handler entry 
                  while ( rxirqs[irqThread.thread.tid] &&
                          rxirqs[irqThread.thread.tid].length ) {

                    const r = rxirqs[irqThread.thread.tid].shift();
                    if ( lastRxIrq !== undefined &&
                         r.ts > nextSnapshot.ts &&
                         r.ts > lastIrq.ts ) {
                         lastRxIrq = r;
                         break;
                    }
                  }
                }
              }
            }
          }

          duration = 0.001;
          queueDuration = 0;
          recvDuration = 0;
          xmitDuration = 0;
          schedDuration = 0;

          queueTS = nextSnapshot.ts;
          xmitTS = nextSnapshot.ts;
          recvTS = nextSnapshot.ts;
          schedTS = nextSnapshot.ts;

          args = { 'key' : nextSnapshot.args.ref,
                   'netcall' : nextSnapshot.args.calling_fct,
                   'queuedby' : undefined,
                 };

          switch (nextSnapshot.args.type) {

            case 'NET_TX':
              label = 'egress'
              state = NETWORKING_STATE.TXQUEUED;

              if (queue) {
                queueTS =  queue.ts;
                queueDuration = nextSnapshot.ts - queueTS;
                duration = queueDuration > 0 ? queueDuration : duration;          
                args['queuedby'] = queue.tid;
              }

              if (j === origSnapshots.length - 1 ) {
                // The next snapshot[0] is the last

                if (lastXmit) {
                  xmitTS = nextSnapshot.ts;
                  schedDuration = xmitTS - schedTS;
                }

                if (lastIrq) {
                  // NET_RX call from threaded-irq handler
                  xmitDuration = (lastIrq.ts + lastIrq.duration) - xmitTS;

                  if (this.threadsByIrqs[lastIrq.key] &&
                      this.threadsByIrqs[lastIrq.key].length) {
                    const irqThread = this.threadsByIrqs[lastIrq.key][0];
                    if ( irqThread ) {
                      args['irqname'] = irqThread.name;
                      args['irqnum'] = irqThread.irq;
                    }
                  }
                }

                if (lastRxIrq) {
                  // NET_RX call from threaded-irq handler
                  //xmitDuration = (lastRxIrq.ts + lastRxIrq.duration) - xmitTS;
                  if (this.threadsByLinuxPid[lastRxIrq.tid])
                    args['netsched'] = this.threadsByLinuxPid[lastRxIrq.tid].name;
                }
              }
              break;

            case 'NET_RX':
              label = 'ingress';
              state = NETWORKING_STATE.RXQUEUED;

              if (nextIrq) {
                queueTS = nextIrq.ts;
                queueDuration = queueTS - nextSnapshot.ts;
                duration = queueDuration > 0 ? queueDuration : duration;
                if (this.threadsByIrqs[nextIrq.key] &&
                    this.threadsByIrqs[nextIrq.key].length) {
                  const irqThread = this.threadsByIrqs[nextIrq.key][0];
                  if ( irqThread ) {
                    args['irqname'] = irqThread.name;
                    args['irqnum'] = irqThread.irq;
                    args['queuedby'] = irqThread.thread.tid;
                  }
                }
              }

              if (j === origSnapshots.length - 1 ) {
                // The next snapshot is the last

                if ( lastRecv )  {
                  // netif_rx_exit or NET_TX direct call from thread
                  recvTS = lastRecv.ts + lastRecv.duration;
                  recvDuration = recvTS - nextSnapshot.ts;
                } else if ( nextIrq && lastRxIrq) {
                  // NET_RX exit call from a threaded-irq handler
                  recvTS = lastRxIrq.ts + lastRxIrq.duration;
                  recvDuration = recvTS - nextSnapshot.ts;
                }
              }
              break;

            case 'XDP_TX':
              label = 'egress'
              duration = 0;
              if (j === origSnapshots.length - 1 ) {
                // The next snapshot is the last
                if (lastXmit) {
                  xmitDuration = lastXmit.ts - nextSnapshot.ts;
                  duration = xmitDuration;
                }
              }
              break;

            case 'XDP_RX':
              label = 'ingress'
              duration = 0;
              if (j === origSnapshots.length - 1 ) {
                // The next snapshot is the last
                if (lastRecv) {
                  recvDuration = lastRecv.ts - nextSnapshot.ts;
                  duration = recvDuration;
                }
              }
              break;

            default:
              break;
          }

          aslice = new tr.model.AsyncSlice(tr.e.net.EthertypeToHuman(nextSnapshot.args.protocol),
             label,
             tr.e.net.getColorForNet(nextSnapshot.args.protocol),
             queueTS,
             nextSnapshot.args,
             duration);
          aslice.important = false;

          if (queueDuration > 0) {
             // AsyncSlice's to show when a RX or TX packet is queued :
             asubslice = new tr.e.net.EthAsyncSlice(
                 state, '',
                 queueTS, args, queueDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration ;
          }
          if (xmitDuration > 0 ) {
            // AsyncSlice's to show when a TX packet is xmited :
            asubslice = new tr.e.net.EthAsyncSlice(
                NETWORKING_STATE.XMIT, '',
                xmitTS, args, xmitDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration + xmitDuration;
          }

          if (recvDuration > 0 ) {
             // AsyncSlice's to show when a  RX packet is recved :
            aslice.duration = queueDuration + recvDuration;
            asubslice = new tr.e.net.EthAsyncSlice(
                 NETWORKING_STATE.RECV, '',
                schedTS, args, recvDuration);
            aslice.subSlices.push(asubslice);
            aslice.duration = queueDuration + recvDuration;
          }

          aslices.push(aslice);

        }

        //thread.asyncSliceGroup.slices.concat(aslices);
        thread.asyncSliceGroup.slices = aslices;
      }, this);

    },

    /**
     * Creates an instance of each registered linux perf event parser.
     * This allows the parsers to register handlers for the events they
     * understand.  We also register our own special handlers (for the
     * timestamp synchronization markers).
     */
    createParsers_() {
      // Instantiate the parsers; this will register handlers for known events
      const allTypeInfos = tr.e.importer.linux_perf.
          Parser.getAllRegisteredTypeInfos();
      const parsers = allTypeInfos.map(
          function(typeInfo) {
            return new typeInfo.constructor(this);
          }, this);

      return parsers;
    },

    registerDefaultHandlers_() {
      this.registerEventHandler('tracing_mark_write',
          FTraceImporter.prototype.traceMarkingWriteEvent_.bind(this));
      // NB: old-style trace markers; deprecated
      this.registerEventHandler('0',
          FTraceImporter.prototype.traceMarkingWriteEvent_.bind(this));
      // Register dummy clock sync handlers to avoid warnings in the log.
      this.registerEventHandler('tracing_mark_write:trace_event_clock_sync',
          function() { return true; });
      this.registerEventHandler('0:trace_event_clock_sync',
          function() { return true; });
    },

    /**
     * Processes a trace_event_clock_sync PTP event.
     */
    traceClockPtpSyncEvent_(eventName, cpuNumber, pid, ts, eventBase) {
      // Check to see if we have a special clock sync marker that contains both
      // the current "ftrace global" time and the 802.1AS time
      //check_clocks-2285    (   2285) [001] ....... 65428.639053: tracing_mark_write: 0: trace_event_clock_sync: parent_ts=1525853974.18952 seq=64884 tai_lat=15850 ep=enp1s0

      event = /parent_ts=(\d+)\.(\d+) seq=(\d+) tai_lat=(.+) ep=(\S+)/.exec(eventBase.details);
      if (!event) return false;
      const pps_num = parseInt(event[3]);
      const tai_offset = parseInt(event[4]);
      const syncep = event[5];

      const pps_tv_sec = parseInt(event[1]);
      const pps_tv_nsec = parseInt(event[2]);
      const TIME = tr.b.UnitScale.TIME;
      const ptpMsTs = tr.b.convertUnit(pps_tv_sec, TIME.SEC, TIME.MILLI_SEC) + tr.b.convertUnit(
              pps_tv_nsec, TIME.NANO_SEC, TIME.MILLI_SEC);
      // A PTP timestamp of zero is used as a sentinel value to indicate
      // that 802.1AS and the ftrace global clock are identical.
      if (ptpMsTs === 0) ptpMsTs = ts;
      const ptpNsTs = tr.b.convertUnit(ptpMsTs, TIME.MILLI_SEC, TIME.SEC);

      if (!this.haveClockSyncedPtpToGlobal_) {
        // We have a clock sync event that contains two timestamps: a timestamp
        // according to the ftrace 'global' clock, and that same timestamp
        // according to clock_gettime(fd_to_clockid(fd_ptp), &ts_ptp)
        /*
        */
        this.model_.clockSyncManager.addClockSyncMarker(
            this.clockDomainId_,
            PTP_TO_FTRACE_GLOBAL_SYNC_ID, ts);
        this.model_.clockSyncManager.addClockSyncMarker(
            tr.model.ClockDomainId.LINUX_CLOCK_PTP,
            PTP_TO_FTRACE_GLOBAL_SYNC_ID, ptpMsTs);
        this.haveClockSyncedPtpToGlobal_ = true;
      }

      const convertFn = this.model_.clockSyncManager.getModelTimeTransformer(
          tr.model.ClockDomainId.LINUX_FTRACE_GLOBAL);

      const ppsMsTs = convertFn(ts);
      const ppsNsTs = tr.b.convertUnit(ppsMsTs, TIME.MILLI_SEC, TIME.SEC);
      const args = {
        'syncep': syncep,
        'ptp_tv_sec': pps_tv_sec,
        'ptp_tv_nsec': pps_tv_nsec,
        'tai_lat_nsec': tai_offset,
        'ftrace_lat_nsec': parseInt(ppsNsTs - ptpNsTs),
        'index': pps_num
      };

      constructor = tr.model.ProcessInstantEvent;
      const parent = this.model_.getOrCreateNetdev(syncep);

      const instantEvent = new constructor('PTP', 'pps',
          tr.b.ColorScheme.getColorIdForReservedName('black'), ppsMsTs,
          args, parent);
      parent.instantEvents.push(instantEvent);

      const counter = parent.
          getOrCreateCounter(null, "ptp-error");
      if (counter.numSeries === 0) {
        counter.addSeries(new tr.model.CounterSeries('pps PTP-time latency [us]',
            ColorScheme.getColorIdForGeneralPurposeString(counter.name)));
      }
      counter.series.forEach(function(series) {
        series.addCounterSample(ppsMsTs, tr.b.convertUnit(
              pps_tv_nsec, TIME.NANO_SEC, TIME.MICRO_SEC));
      });

      const tai_counter = parent.
          getOrCreateCounter(null, "tai-error");
      if (tai_counter.numSeries === 0) {
        tai_counter.addSeries(new tr.model.CounterSeries('pps TAI-time latency [us]',
            ColorScheme.getColorIdForGeneralPurposeString(tai_counter.name)));
      }
      tai_counter.series.forEach(function(series) {
        series.addCounterSample(ppsMsTs, tr.b.convertUnit(
              tai_offset, TIME.NANO_SEC, TIME.MICRO_SEC));
      });

      return true;
    },

    /**
     * Processes a trace_event_clock_sync event.
     */
    traceClockSyncEvent_(eventName, cpuNumber, pid, ts, eventBase) {
      // Check to see if we have a normal clock sync marker that contains a
      // sync ID and the current time according to the "ftrace global" clock.
      let event = /name=(\w+?)\s(.+)/.exec(eventBase.details);
      if (event) {
        // TODO(alexandermont): This section of code seems to be broken. It
        // creates an "args" variable, but doesn't seem to do anything with it.
        const name = event[1];
        const pieces = event[2].split(' ');
        const args = {
          perfTs: ts
        };
        for (let i = 0; i < pieces.length; i++) {
          const parts = pieces[i].split('=');
          if (parts.length !== 2) {
            throw new Error('omgbbq');
          }
          args[parts[0]] = parts[1];
        }

        this.model_.clockSyncManager.addClockSyncMarker(
            this.clockDomainId_, name, ts);
        return true;
      }

      // Check to see if we have a "new style" clock sync marker that contains
      // only a sync ID.
      event = /name=([\w\-]+)/.exec(eventBase.details);
      if (event) {
        this.model_.clockSyncManager.addClockSyncMarker(
            this.clockDomainId_, event[1], ts);
        return true;
      }

      // Check to see if we have a special clock sync marker that contains both
      // the current "ftrace global" time and the current CLOCK_MONOTONIC time.
      event = /parent_ts=(\d+\.?\d*)/.exec(eventBase.details);
      if (!event) return false;

      let monotonicTs = event[1] * 1000;
      // A monotonic timestamp of zero is used as a sentinel value to indicate
      // that CLOCK_MONOTONIC and the ftrace global clock are identical.
      if (monotonicTs === 0) monotonicTs = ts;

      if (this.haveClockSyncedMonotonicToGlobal_) {
        // ftrace sometimes includes multiple clock syncs between the monotonic
        // and global clocks within a single trace. We protect against this by
        // only taking the first one into account.
        return true;
      }

      // We have a clock sync event that contains two timestamps: a timestamp
      // according to the ftrace 'global' clock, and that same timestamp
      // according to clock_gettime(CLOCK_MONOTONIC).
      this.model_.clockSyncManager.addClockSyncMarker(
          this.clockDomainId_,
          MONOTONIC_TO_FTRACE_GLOBAL_SYNC_ID, ts);
      this.model_.clockSyncManager.addClockSyncMarker(
          tr.model.ClockDomainId.LINUX_CLOCK_MONOTONIC,
          MONOTONIC_TO_FTRACE_GLOBAL_SYNC_ID, monotonicTs);

      this.haveClockSyncedMonotonicToGlobal_ = true;
      return true;
    },

    /**
     * Processes a trace_marking_write event.
     */
    traceMarkingWriteEvent_(eventName, cpuNumber, pid, ts, eventBase,
        threadName) {
      // Some profiles end up with a \n\ on the end of each line. Strip it
      // before we do the comparisons.
      eventBase.details = eventBase.details.replace(/\\n.*$/, '');

      const event = /^\s*(\w+):\s*(.*)$/.exec(eventBase.details);
      if (!event) {
        // Check if the event matches events traced by the Android framework
        const tag = eventBase.details.substring(0, 2);
        if (tag === 'B|' || tag === 'E' || tag === 'E|' || tag === 'X|' ||
            tag === 'C|' || tag === 'S|' || tag === 'F|') {
          eventBase.subEventName = 'android';
        } else {
          return false;
        }
      } else {
        eventBase.subEventName = event[1];
        eventBase.details = event[2];
      }

      const writeEventName = eventName + ':' + eventBase.subEventName;
      const handler = this.eventHandlers_[writeEventName];
      if (!handler) {
        this.model_.importWarning({
          type: 'parse_error',
          message: 'Unknown trace_marking_write event ' + writeEventName
        });
        return true;
      }
      return handler(writeEventName, cpuNumber, pid, ts, eventBase, threadName);
    },

    /**
     * Walks the this.events_ structure and creates Cpu objects.
     */
    importCpuData_(modelTimeTransformer) {
      this.forEachLine_(function(text, eventBase, cpuNumber, pid, ts) {
        const eventName = eventBase.eventName;
        const handler = this.eventHandlers_[eventName];
        if (!handler) {
          this.model_.importWarning({
            type: 'parse_error',
            message: 'Unknown event ' + eventName + ' (' + text + ')'
          });
          return;
        }
        ts = modelTimeTransformer(ts);
        if (!handler(eventName, cpuNumber, pid, ts, eventBase)) {
          this.model_.importWarning({
            type: 'parse_error',
            message: 'Malformed ' + eventName + ' event (' + text + ')'
          });
        }
      }.bind(this));
    },

    /**
     * Walks the this.events_ structure and populates this.lines_.
     */
    parseLines_() {
      let extractResult = FTraceImporter._extractEventsFromSystraceHTML(
          this.events_, true);
      if (!extractResult.ok) {
        extractResult = FTraceImporter._extractEventsFromSystraceMultiHTML(
            this.events_, true);
      }
      let lineParser = undefined;
      if (extractResult.ok) {
        for (const line of extractResult.lines) {
          lineParser = this.parseLine_(line, lineParser);
        }
      } else {
        const r = new tr.importer.SimpleLineReader(this.events_);
        for (const line of r) {
          lineParser = this.parseLine_(line, lineParser);
        }
      }
    },

    parseLine_(line, lineParser) {
      line = line.trim();
      if (line.length === 0) return lineParser;
      if (/^#/.test(line)) {
        const clockType = /^# clock_type=([A-Z_]+)$/.exec(line);
        // This allows the clock domain to be specified through a comment,
        // Ex. "# clock_type=LINUX_CLOCK_MONOTONIC".
        // This is used in the WALT trace agent.
        if (clockType) {
          this.clockDomainId_ = clockType[1];
        }
        return lineParser;
      }

      if (!lineParser) {
        lineParser = autoDetectLineParser(line);
        if (!lineParser) {
          this.model_.importWarning({
            type: 'parse_error',
            message: 'Cannot parse line: ' + line
          });
          return lineParser;
        }
      }

      const eventBase = lineParser(line);
      if (!eventBase) {
        this.model_.importWarning({
          type: 'parse_error',
          message: 'Unrecognized line: ' + line
        });
        return lineParser;
      }

      this.lines_.push([
        line,
        eventBase,
        parseInt(eventBase.cpuNumber),
        parseInt(eventBase.pid),
	      parseFloat(eventBase.timestamp) * 1000
      ]);
      return lineParser;
    },

    /**
     * Calls |handler| for every parsed line.
     */
    forEachLine_(handler) {
      for (let i = 0; i < this.lines_.length; ++i) {
        const line = this.lines_[i];
        handler.apply(this, line);
      }
    },

    /**
     * Initializes the ftrace importer. This initialization can't be done in the
     * constructor because all trace event handlers may not have been registered
     * by that point.
     */
    lazyInit_() {
      this.parsers_ = this.createParsers_();
      this.registerDefaultHandlers_();
      this.parseLines_();
    }
  };

  tr.importer.Importer.register(FTraceImporter);

  return {
    FTraceImporter,
    _FTraceImporterTestExports: TestExports,
    IMPORT_PRIORITY,
  };
});
</script>
